{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b42b18",
   "metadata": {},
   "source": [
    "# Chapter 6: Efficient LLM Serving with vLLM and PagedAttention\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ayoisio/genai-on-google-cloud/blob/main/chapter-6/colabs/03_vllm_serving.ipynb)\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "- Understand PagedAttention and its impact on LLM serving efficiency\n",
    "- Configure vLLM for different bottleneck patterns (bandwidth, memory, compute)\n",
    "- Implement batch inference and continuous batching\n",
    "- Use prefix caching to optimize repeated prompt scenarios\n",
    "- Benchmark throughput and analyze GPU memory utilization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A Colab environment with GPU (T4, L4, or A100 recommended)\n",
    "- Basic understanding of LLM inference and GPU memory concepts\n",
    "- Familiarity with Python and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf6b52",
   "metadata": {},
   "source": [
    "## The Serving Efficiency Problem\n",
    "\n",
    "Traditional LLM serving wastes memory through **KV cache fragmentation**:\n",
    "\n",
    "```\n",
    "Traditional Approach:\n",
    "┌────────────────────────────────────────────────┐\n",
    "│  Request 1: [████████░░░░░░░░░░░░░░░░░░░░░░]  │  Allocated: 512 tokens\n",
    "│  Request 2: [██████████████░░░░░░░░░░░░░░░░]  │  Used: 50-200 tokens\n",
    "│  Request 3: [████░░░░░░░░░░░░░░░░░░░░░░░░░░]  │  Wasted: 60-90%\n",
    "└────────────────────────────────────────────────┘\n",
    "\n",
    "PagedAttention:\n",
    "┌────────────────────────────────────────────────┐\n",
    "│  [██][██][██][██][░░][██][██][░░][██][██][██]  │  Dynamic pages\n",
    "│  Req1  Req2  Req1  Req3     Req2     Req1-3   │  Near 100% utilization\n",
    "└────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "This memory waste directly limits concurrent requests and throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880d215",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"✓ GPU Available: {gpu_name}\")\n",
    "    print(f\"✓ GPU Memory: {gpu_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠ No GPU detected!\")\n",
    "    print(\"Enable GPU: Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM (requires version 0.4.0+ for prefix_caching)\n",
    "!pip install -q \"vllm>=0.4.0\"\n",
    "print(\"✓ vLLM installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print(\"✓ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb03cb3a",
   "metadata": {},
   "source": [
    "## 2. Understanding vLLM Configuration\n",
    "\n",
    "vLLM parameters map directly to the bottleneck patterns from Chapter 6:\n",
    "\n",
    "| Parameter | Bottleneck | Description |\n",
    "|-----------|------------|-------------|\n",
    "| `tensor_parallel_size` | Pattern 2 (Memory) | Split model across GPUs |\n",
    "| `gpu_memory_utilization` | Pattern 2 (Memory) | % of GPU memory for KV cache |\n",
    "| `max_num_seqs` | Pattern 1 (Bandwidth) | Max concurrent sequences |\n",
    "| `enable_prefix_caching` | Pattern 1 (Bandwidth) | Cache common prompt prefixes |\n",
    "| `max_model_len` | Pattern 2 (Memory) | Maximum context length |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f8463",
   "metadata": {},
   "source": [
    "## 3. Load a Model with vLLM\n",
    "\n",
    "We'll use a small model that fits on Colab's GPU for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a38957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection based on available GPU memory\n",
    "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "if gpu_memory_gb >= 40:  # A100 or similar\n",
    "    MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "    GPU_MEMORY_UTIL = 0.9\n",
    "    MAX_MODEL_LEN = 8192\n",
    "elif gpu_memory_gb >= 20:  # L4 or T4\n",
    "    MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "    GPU_MEMORY_UTIL = 0.9\n",
    "    MAX_MODEL_LEN = 4096\n",
    "else:  # Smaller GPU\n",
    "    MODEL_ID = \"microsoft/phi-2\"\n",
    "    GPU_MEMORY_UTIL = 0.85\n",
    "    MAX_MODEL_LEN = 2048\n",
    "\n",
    "print(f\"Selected model: {MODEL_ID}\")\n",
    "print(f\"GPU memory utilization: {GPU_MEMORY_UTIL}\")\n",
    "print(f\"Max context length: {MAX_MODEL_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vLLM with production-optimized settings\n",
    "print(f\"Loading {MODEL_ID} with vLLM...\")\n",
    "print(\"This may take a few minutes for initial download...\")\n",
    "print()\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_ID,\n",
    "    tensor_parallel_size=1,  # Single GPU\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTIL,\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    enable_prefix_caching=True,  # Enable prefix caching for efficiency\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106762af",
   "metadata": {},
   "source": [
    "## 4. Basic Inference with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "# Single prompt inference\n",
    "prompt = \"Explain the key benefits of PagedAttention in LLM serving:\"\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(\"Response:\")\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(f\"\\nLatency: {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61779c69",
   "metadata": {},
   "source": [
    "## 5. Batch Inference - Continuous Batching Demo\n",
    "\n",
    "vLLM's continuous batching handles variable-length outputs efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of prompts with different expected output lengths\n",
    "batch_prompts = [\n",
    "    \"What is 2+2?\",  # Very short response expected\n",
    "    \"Explain quantum computing in one paragraph.\",  # Medium response\n",
    "    \"Write a detailed comparison of GPUs vs TPUs for machine learning training, covering architecture, performance, cost, and use cases.\",  # Long response\n",
    "    \"Name three programming languages.\",  # Short response\n",
    "    \"What is the capital of France?\",  # Very short response\n",
    "    \"Describe the transformer architecture and its key innovations.\",  # Medium-long response\n",
    "]\n",
    "\n",
    "print(f\"Processing batch of {len(batch_prompts)} prompts...\")\n",
    "print()\n",
    "\n",
    "# Time batch inference\n",
    "start_time = time.time()\n",
    "outputs = llm.generate(batch_prompts, sampling_params)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "total_tokens = sum(len(o.outputs[0].token_ids) for o in outputs)\n",
    "tokens_per_second = total_tokens / total_time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BATCH INFERENCE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    response = output.outputs[0].text\n",
    "    num_tokens = len(output.outputs[0].token_ids)\n",
    "    print(f\"\\nPrompt {i+1}: {batch_prompts[i][:50]}...\")\n",
    "    print(f\"Response ({num_tokens} tokens): {response[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Total time: {total_time:.2f}s\")\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Throughput: {tokens_per_second:.2f} tokens/second\")\n",
    "print(f\"Average latency per request: {total_time/len(batch_prompts):.2f}s\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35709243",
   "metadata": {},
   "source": [
    "## 6. Prefix Caching Demo\n",
    "\n",
    "Prefix caching dramatically speeds up requests that share common system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common system prompt (like in chat applications)\n",
    "system_prompt = \"\"\"You are a helpful AI assistant specialized in cloud computing and machine learning infrastructure. \n",
    "You provide concise, accurate answers focused on practical implementation.\n",
    "Always consider cost, performance, and scalability in your recommendations.\n",
    "\n",
    "User: \"\"\"\n",
    "\n",
    "# Multiple queries with the same prefix\n",
    "user_queries = [\n",
    "    \"What GPU should I use for fine-tuning a 7B model?\",\n",
    "    \"How do I reduce serving latency for my LLM application?\",\n",
    "    \"When should I choose TPUs over GPUs?\",\n",
    "    \"What is the best storage option for training data?\",\n",
    "    \"How can I reduce cold start times for my model?\",\n",
    "]\n",
    "\n",
    "# Create full prompts with shared prefix\n",
    "full_prompts = [system_prompt + query for query in user_queries]\n",
    "\n",
    "print(\"Testing prefix caching with shared system prompt...\")\n",
    "print(f\"System prompt length: {len(system_prompt)} characters\")\n",
    "print()\n",
    "\n",
    "# First run - cache population\n",
    "start_time = time.time()\n",
    "outputs_first = llm.generate(full_prompts[:2], sampling_params)\n",
    "first_run_time = time.time() - start_time\n",
    "\n",
    "# Second run - should benefit from cached prefix\n",
    "start_time = time.time()\n",
    "outputs_second = llm.generate(full_prompts[2:], sampling_params)\n",
    "second_run_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREFIX CACHING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"First batch (2 queries): {first_run_time:.2f}s\")\n",
    "print(f\"Second batch (3 queries): {second_run_time:.2f}s\")\n",
    "print(f\"Time per query (first): {first_run_time/2:.2f}s\")\n",
    "print(f\"Time per query (second): {second_run_time/3:.2f}s\")\n",
    "print()\n",
    "print(\"Note: Second batch may be faster due to prefix caching\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4fbb1",
   "metadata": {},
   "source": [
    "## 7. Memory and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc867ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory analysis\n",
    "import torch\n",
    "\n",
    "def get_gpu_memory_info():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        return {\n",
    "            'allocated_gb': allocated,\n",
    "            'reserved_gb': reserved,\n",
    "            'total_gb': total,\n",
    "            'free_gb': total - reserved,\n",
    "            'utilization_pct': (reserved / total) * 100\n",
    "        }\n",
    "    return None\n",
    "\n",
    "mem_info = get_gpu_memory_info()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU MEMORY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total GPU Memory:     {mem_info['total_gb']:.2f} GB\")\n",
    "print(f\"Reserved Memory:      {mem_info['reserved_gb']:.2f} GB\")\n",
    "print(f\"Allocated Memory:     {mem_info['allocated_gb']:.2f} GB\")\n",
    "print(f\"Free Memory:          {mem_info['free_gb']:.2f} GB\")\n",
    "print(f\"Utilization:          {mem_info['utilization_pct']:.1f}%\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"vLLM Memory Management:\")\n",
    "print(f\"  - gpu_memory_utilization set to: {GPU_MEMORY_UTIL}\")\n",
    "print(f\"  - This reserves {GPU_MEMORY_UTIL * 100:.0f}% of GPU memory for KV cache\")\n",
    "print(f\"  - Remaining {(1-GPU_MEMORY_UTIL) * 100:.0f}% is headroom for spikes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde544d",
   "metadata": {},
   "source": [
    "## 8. Throughput Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e939c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with different batch sizes\n",
    "import statistics\n",
    "\n",
    "def benchmark_throughput(llm, num_prompts, num_runs=3):\n",
    "    \"\"\"Benchmark throughput with specified number of prompts.\"\"\"\n",
    "    \n",
    "    # Generate test prompts\n",
    "    test_prompts = [\n",
    "        f\"Write a brief explanation of concept number {i} in machine learning.\"\n",
    "        for i in range(num_prompts)\n",
    "    ]\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    \n",
    "    latencies = []\n",
    "    token_counts = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        outputs = llm.generate(test_prompts, sampling_params)\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        total_tokens = sum(len(o.outputs[0].token_ids) for o in outputs)\n",
    "        \n",
    "        latencies.append(latency)\n",
    "        token_counts.append(total_tokens)\n",
    "    \n",
    "    avg_latency = statistics.mean(latencies)\n",
    "    avg_tokens = statistics.mean(token_counts)\n",
    "    throughput = avg_tokens / avg_latency\n",
    "    \n",
    "    return {\n",
    "        'num_prompts': num_prompts,\n",
    "        'avg_latency_s': avg_latency,\n",
    "        'avg_tokens': avg_tokens,\n",
    "        'throughput_tps': throughput,\n",
    "        'latency_per_prompt_s': avg_latency / num_prompts,\n",
    "    }\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"THROUGHPUT BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "batch_sizes = [1, 4, 8, 16]\n",
    "results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Benchmarking batch size {batch_size}...\")\n",
    "    result = benchmark_throughput(llm, batch_size, num_runs=2)\n",
    "    results.append(result)\n",
    "\n",
    "print()\n",
    "print(\"Results:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Batch Size':<12} {'Latency (s)':<14} {'Throughput (tok/s)':<20} {'Per-Request (s)':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['num_prompts']:<12} {r['avg_latency_s']:<14.2f} {r['throughput_tps']:<20.2f} {r['latency_per_prompt_s']:<15.2f}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "print(\"Key insight: Larger batches improve throughput (tokens/second)\")\n",
    "print(\"but may increase individual request latency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1a7dc",
   "metadata": {},
   "source": [
    "## 9. Configuration for Different Bottleneck Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a04fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration recommendations based on bottleneck patterns\n",
    "\n",
    "configs = {\n",
    "    \"Pattern 1 - Bandwidth Bottleneck\": {\n",
    "        \"symptom\": \"GPU utilization 50-70%, not improving with optimization\",\n",
    "        \"config\": {\n",
    "            \"gpu_memory_utilization\": 0.9,\n",
    "            \"enable_prefix_caching\": True,\n",
    "            \"max_num_seqs\": 256,\n",
    "        },\n",
    "        \"explanation\": \"Maximize memory usage and cache prefixes to reduce data movement\"\n",
    "    },\n",
    "    \"Pattern 2 - Memory Bottleneck\": {\n",
    "        \"symptom\": \"OOM errors, model doesn't fit on GPU\",\n",
    "        \"config\": {\n",
    "            \"gpu_memory_utilization\": 0.85,\n",
    "            \"tensor_parallel_size\": 2,\n",
    "            \"max_num_seqs\": 64,\n",
    "        },\n",
    "        \"explanation\": \"Reduce memory pressure, split model across GPUs if needed\"\n",
    "    },\n",
    "    \"Pattern 3 - Compute Bottleneck\": {\n",
    "        \"symptom\": \"Sustained 100% GPU utilization\",\n",
    "        \"config\": {\n",
    "            \"gpu_memory_utilization\": 0.9,\n",
    "            \"enable_prefix_caching\": True,\n",
    "        },\n",
    "        \"explanation\": \"Already optimized - need faster hardware or more GPUs\"\n",
    "    },\n",
    "    \"Pattern 4 - Network Bottleneck\": {\n",
    "        \"symptom\": \"Multi-GPU scaling shows diminishing returns\",\n",
    "        \"config\": {\n",
    "            \"tensor_parallel_size\": \"minimum needed to fit model\",\n",
    "        },\n",
    "        \"explanation\": \"Minimize inter-GPU communication, use NVLink if available\"\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"vLLM CONFIGURATION BY BOTTLENECK PATTERN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for pattern, info in configs.items():\n",
    "    print(f\"\\n{pattern}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Symptom: {info['symptom']}\")\n",
    "    print(f\"Recommended config: {info['config']}\")\n",
    "    print(f\"Why: {info['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6e562",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae260c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "print(\"Cleaning up...\")\n",
    "\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "mem_info = get_gpu_memory_info()\n",
    "print(f\"GPU memory after cleanup: {mem_info['reserved_gb']:.2f} GB reserved\")\n",
    "print(\"✓ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03c95a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key vLLM Optimizations\n",
    "\n",
    "| Feature | Impact | When to Use |\n",
    "|---------|--------|-------------|\n",
    "| **PagedAttention** | 2-3x throughput | Always (automatic) |\n",
    "| **Continuous Batching** | 85-90% GPU utilization | High-traffic scenarios |\n",
    "| **Prefix Caching** | Faster repeated prompts | Chat applications, agents |\n",
    "| **Tensor Parallelism** | Fit large models | When model > GPU memory |\n",
    "\n",
    "### Production Configuration Checklist\n",
    "\n",
    "1. **Set `gpu_memory_utilization`** to 0.85-0.9 (leave headroom for spikes)\n",
    "2. **Enable `prefix_caching`** for conversational workloads\n",
    "3. **Use minimum `tensor_parallel_size`** that fits your model\n",
    "4. **Tune `max_num_seqs`** based on your latency requirements\n",
    "5. **Monitor GPU utilization** to identify your bottleneck pattern\n",
    "\n",
    "### Next Steps & Learning Labs\n",
    "\n",
    "| Resource | Description |\n",
    "|----------|-------------|\n",
    "| [vLLM Official Documentation](https://docs.vllm.ai/) | Complete guide to vLLM configuration and deployment |\n",
    "| [vLLM Recipes & Tutorials](https://recipes.vllm.ai/) | Example notebooks and step-by-step tutorials |\n",
    "| [Model Garden vLLM Tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_vllm_text_only_tutorial.ipynb) | Deploy models with vLLM on Vertex AI |\n",
    "| [Gemma Deployment on GKE](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_deployment_on_gke.ipynb) | Deploy Gemma on GKE using GPU |\n",
    "| [PagedAttention Paper](https://arxiv.org/abs/2309.06180) | Original research paper on PagedAttention |\n",
    "| [vLLM Performance Benchmarks](https://perf.vllm.ai/) | Compare vLLM performance across configurations |\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- [02_model_garden_deployment.ipynb](./02_model_garden_deployment.ipynb) - Deploy models from Vertex AI Model Garden\n",
    "- [01_gemma_finetuning.ipynb](./01_gemma_finetuning.ipynb) - Fine-tune Gemma with QLoRA"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
