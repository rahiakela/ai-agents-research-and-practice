{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed50ab15",
   "metadata": {
    "id": "ed50ab15"
   },
   "source": [
    "# Chapter 6: Fine-Tuning Gemma with QLoRA on Vertex AI\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ayoisio/genai-on-google-cloud/blob/main/chapter-6/colabs/01_gemma_finetuning.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to fine-tune the Gemma 7B-it model using QLoRA (Quantized Low-Rank Adaptation) on Google Colab with Vertex AI. We will train the model to act as a financial analyst, summarizing news excerpts into a specific JSON format.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Configure QLoRA for efficient fine-tuning on consumer GPUs\n",
    "- Fine-tune Gemma 7B using the HuggingFace transformers and PEFT libraries\n",
    "- Evaluate fine-tuned vs. base model performance\n",
    "- Save and deploy the fine-tuned model to Google Cloud Storage\n",
    "- Understand production deployment options on GCP\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Google Colab with GPU runtime (T4, L4, or A100 recommended)\n",
    "- Google Cloud Project with Vertex AI API enabled\n",
    "- HuggingFace account with access to Gemma (free, one-time setup)\n",
    "- Dataset file (`dataset.jsonl`) for training\n",
    "\n",
    "> **See also**: [Gemma Model Documentation](https://ai.google.dev/gemma) | [QLoRA Paper](https://arxiv.org/abs/2305.14314)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d4676",
   "metadata": {
    "id": "203d4676"
   },
   "source": [
    "## 1. Setup and Authentication\n",
    "\n",
    "First, we'll authenticate with Google Cloud and set up the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f54030",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55f54030",
    "outputId": "889127f4-4798-41ff-81fe-207a9a765aee"
   },
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "else:\n",
    "    print(\"Not running in Google Colab. This notebook is optimized for Colab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f933e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c3f933e",
    "outputId": "5d767587-a57a-46fb-c7be-3e3e5daf46b5"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "if IN_COLAB:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    print(\"\u2713 Authentication successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c890e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c7c890e",
    "outputId": "abfcbfe4-0780-4aaa-b5d5-4f18af42a74e"
   },
   "outputs": [],
   "source": [
    "# Set GCP Project ID and Region\n",
    "import os\n",
    "\n",
    "# Prompt for Project ID\n",
    "PROJECT_ID = input(\"Enter your GCP Project ID: \")\n",
    "REGION = input(\"Enter your GCP Region (e.g., us-central1): \")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_REGION'] = REGION\n",
    "\n",
    "print(f\"\\n\u2713 Project ID: {PROJECT_ID}\")\n",
    "print(f\"\u2713 Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d33245",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92d33245",
    "outputId": "a4b709be-b716-4d8f-94fe-fc2e2d35efc3"
   },
   "outputs": [],
   "source": [
    "# Configure gcloud\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "!gcloud config set ai/region {REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b1f94",
   "metadata": {
    "id": "d04b1f94"
   },
   "source": [
    "## 1.1. Initialize Vertex AI\n",
    "\n",
    "Initialize Vertex AI SDK to access Model Garden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77312d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d77312d2",
    "outputId": "1c793b9c-2465-48e2-da16-1a0a6c1d88a7"
   },
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(f\"\u2713 Vertex AI initialized\")\n",
    "print(f\"  Project: {PROJECT_ID}\")\n",
    "print(f\"  Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63680e91",
   "metadata": {
    "id": "63680e91"
   },
   "source": [
    "## 1.2. Enable Required APIs\n",
    "\n",
    "Enable Vertex AI and related APIs for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322206b",
   "metadata": {
    "id": "e322206b"
   },
   "outputs": [],
   "source": [
    "# # Enable required APIs\n",
    "# print(\"Enabling required GCP APIs...\")\n",
    "# print(\"This may take a minute...\\n\")\n",
    "\n",
    "# # Enable Vertex AI API\n",
    "# !gcloud services enable aiplatform.googleapis.com --project={PROJECT_ID}\n",
    "\n",
    "# # Enable Cloud Storage API\n",
    "# !gcloud services enable storage.googleapis.com --project={PROJECT_ID}\n",
    "\n",
    "# # Enable Compute Engine API (for GPU resources)\n",
    "# !gcloud services enable compute.googleapis.com --project={PROJECT_ID}\n",
    "\n",
    "# print(\"\\n\u2713 All required APIs enabled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14671f2",
   "metadata": {
    "id": "e14671f2"
   },
   "source": [
    "## 2. Check GPU Availability\n",
    "\n",
    "Make sure you're using a GPU runtime. Go to **Runtime > Change runtime type** and select a GPU (T4, L4, or A100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537355f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a537355f",
    "outputId": "de72b77d-5846-4112-b14b-e46f5ec2c8c5"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\u2713 GPU Available: {gpu_name}\")\n",
    "    print(f\"\u2713 GPU Memory: {gpu_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No GPU detected! Please enable GPU in Runtime > Change runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb2bd8",
   "metadata": {
    "id": "2ebb2bd8"
   },
   "source": [
    "## 3. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a17dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "222a17dc",
    "outputId": "7158ea13-3724-4ec7-a467-4f1c318a05a4"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q bitsandbytes transformers peft trl accelerate datasets google-cloud-aiplatform huggingface_hub\n",
    "print(\"\u2713 All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b22c8b",
   "metadata": {
    "id": "23b22c8b"
   },
   "source": [
    "## 4. Upload Dataset\n",
    "\n",
    "Upload your `dataset.jsonl` file using the file browser on the left, or use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b42f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "c70b42f2",
    "outputId": "0771885b-8a45-4c5e-ab70-4b4a58aff4c2"
   },
   "outputs": [],
   "source": [
    "# Optional: Upload dataset file\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists('dataset.jsonl'):\n",
    "        print(\"Please upload your dataset.jsonl file:\")\n",
    "        uploaded = files.upload()\n",
    "        print(\"\u2713 Dataset uploaded successfully!\")\n",
    "    else:\n",
    "        print(\"\u2713 Dataset file found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b2bb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "705b2bb1",
    "outputId": "26495755-b9d5-4d0f-903b-1b7df0e873f4"
   },
   "outputs": [],
   "source": [
    "# Verify dataset\n",
    "import json\n",
    "\n",
    "dataset_path = \"dataset.jsonl\"\n",
    "\n",
    "with open(dataset_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    num_examples = len(lines)\n",
    "    print(f\"\u2713 Dataset loaded: {num_examples} examples\")\n",
    "\n",
    "    # Show first example\n",
    "    if num_examples > 0:\n",
    "        first_example = json.loads(lines[0])\n",
    "        print(\"\\nFirst example:\")\n",
    "        print(first_example['text'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ac9f1",
   "metadata": {
    "id": "7f1ac9f1"
   },
   "source": [
    "## 5. Fine-Tuning Configuration and Training\n",
    "\n",
    "Now we'll configure and start the fine-tuning process using QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55158623",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55158623",
    "outputId": "ee473f32-a285-41aa-dc1b-95d9ec225b29"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c843d3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c843d3b",
    "outputId": "33ae9c81-8e59-4105-c19e-32dc9514a6b5"
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "# Model weights: HuggingFace (via authentication above)\n",
    "# Deployment: Vertex AI Model Registry (configured earlier)\n",
    "\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "output_dir = \"./gemma-7b-analyst\"\n",
    "adapter_name = \"gemma-7b-analyst-adapter\"\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Adapter name: {adapter_name}\")\n",
    "print()\n",
    "print(\"\u2713 Model will be loaded from HuggingFace\")\n",
    "print(\"\u2713 Fine-tuned model will be deployed to Vertex AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68839268",
   "metadata": {
    "id": "68839268"
   },
   "source": [
    "### 5.1. Authenticate with HuggingFace for Gemma Access\n",
    "\n",
    "While we use Vertex AI for deployment, Gemma model weights are accessed via HuggingFace. This is a simple one-time authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813a405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3813a405",
    "outputId": "c5c607d5-2d72-4f13-828c-dbe5f2b7fa5f"
   },
   "outputs": [],
   "source": [
    "# Authenticate with HuggingFace to access Gemma\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HUGGINGFACE AUTHENTICATION\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"To access Gemma model weights, you need a HuggingFace account.\")\n",
    "print()\n",
    "print(\"Steps:\")\n",
    "print(\"1. Go to https://huggingface.co/google/gemma-7b-it\")\n",
    "print(\"2. Click 'Agree and access repository' (one-time)\")\n",
    "print(\"3. Go to https://huggingface.co/settings/tokens\")\n",
    "print(\"4. Create a new token (Read access is sufficient)\")\n",
    "print(\"5. Copy and paste it below\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "HF_TOKEN = input(\"Enter your HuggingFace token: \")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "\n",
    "print(\"\\n\u2713 Authenticated with HuggingFace!\")\n",
    "print(\"\u2713 You now have access to Gemma model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd26ee6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcd26ee6",
    "outputId": "71950e08-ab3e-48dc-cbbe-37bd749ff85c"
   },
   "outputs": [],
   "source": [
    "# Alternative: Use Kaggle instead of HuggingFace (OPTIONAL)\n",
    "# If you prefer Kaggle over HuggingFace, uncomment and run this cell instead\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "#\n",
    "# print(\"KAGGLE AUTHENTICATION (Alternative to HuggingFace)\")\n",
    "# print(\"Get credentials from: https://www.kaggle.com/settings -> API -> Create New Token\")\n",
    "# print()\n",
    "#\n",
    "# KAGGLE_USERNAME = input(\"Kaggle username: \")\n",
    "# KAGGLE_KEY = input(\"Kaggle API key: \")\n",
    "#\n",
    "# os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n",
    "# os.environ['KAGGLE_KEY'] = KAGGLE_KEY\n",
    "#\n",
    "# os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "# with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "#     json.dump({\"username\": KAGGLE_USERNAME, \"key\": KAGGLE_KEY}, f)\n",
    "#\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n",
    "# print(\"\u2713 Kaggle configured!\")\n",
    "#\n",
    "# # Then use Kaggle model path in model loading:\n",
    "# # model_name = \"kaggle://google/gemma/pyTorch/7b-it\"\n",
    "\n",
    "print(\"SKIP THIS CELL - Use HuggingFace authentication (next section) unless you prefer Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1f44e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7df1f44e",
    "outputId": "7abd5d74-e723-48e6-e6f0-27ba81b22a48"
   },
   "outputs": [],
   "source": [
    "# 1. QLoRA Configuration (4-bit quantization)\n",
    "print(\"Configuring 4-bit quantization...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Quantization config ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb5ab4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661,
     "referenced_widgets": [
      "0c9b36a627a04ff89ba620a278a0c33c",
      "602e0d72d6a74c7383357f7722b59432",
      "acad8440087f410482851ddc3617daae",
      "7761039b168544dab3b37f845cfe1d85",
      "1ff9c691d2644736942f6f42e005a681",
      "871cf749e9bc4d80a7fa82b54806137a",
      "7e095df629744c29b6f3b2a8aeb31b77",
      "3258c5c3ed014e37a7fed5426f911aa2",
      "73439b7c1f714cd69518dc035dee7999",
      "e479074cb58546daa1509388bd7687f9",
      "2a32a805998c48a3adfedd7a04ad5751",
      "9bfc26620db145e3a7e37476e74d2235",
      "c937e90f2a77498ca8381c3206d88eea",
      "3e6d5b7978aa48489ec42ebbf782cb09",
      "335e3fb8fa5347098f8aeba0237beb4f",
      "8b32b663d51c4c9aaaa9ddad35c1b477",
      "18a09fe34d2a4952bc37508ef8785ebd",
      "877b74ffaf884f11b4b26fc52c6db53c",
      "ce9a6a3c0cad4c068b26badaad3fe034",
      "33952a6727b845cea2b892f53d415498",
      "d8cebef42c37413aaf12091e13c722bc",
      "24e1eed0fdfb4690acb8b47ecb153107",
      "24882cab963a4269a7dafdb7d457f9e6",
      "96c31bdb586149f288ae3324c19189ca",
      "b35e0a076c5041fd86c111adc8be8c9f",
      "f95b6d82e78f42bf9bd6b0c9358c5534",
      "5693ed83c9044bf19f24f567167f92bc",
      "ff32b4fca0364c9e8ccc473704768867",
      "aa935165a78746f097244afb803c1bc4",
      "6c3f65852e714bc8986dd81b794f34dd",
      "d7b38057d88a4d28b174833e1c7d9d04",
      "1bfbbb0374274c49be388dbbdfce20cd",
      "6eef8d9b158249c3bd07dbe2113fb184",
      "e06704bc748b4795ac2a419318e6163a",
      "2c0979a44ae7476bbfb21f9ac90e3894",
      "4b6053b45c3045b6844d828bf0943571",
      "20a8686a5e7340a8b1149deedbec62be",
      "5891d8e59f2647269b9bd134eb538789",
      "900e33b07b1b4104bf81bc67a7c56813",
      "ce4db386a62a4d85ad1f5fac0249c817",
      "6dd2817011d34af38bf76e1d7470f6e1",
      "d978545daf174733a9ed0991b0d8a1db",
      "205b3dea1dfc4728b4f916d0462015d2",
      "52eb03d283814ed58cfdd6c7cec0aae8",
      "4e2f4e88dce840769130066cd6f54b34",
      "395a00236ada4afcb60ecd871bc52edd",
      "562bdaba5c8b4172bd0245994ca8d5b7",
      "a48699e163cd4466b37fb0999b93e67d",
      "18a5d40c8d9443cebcd0a7f7d456adbc",
      "60647759d39c41d2a0dd2a83d967f614",
      "db9bdada7e9240919643d0000fb54e5e",
      "250e6bdfdf004262af5119261c5fc7fd",
      "24dccf003b8c456780cb2423bd62bf0d",
      "f4b564a06bdb444e856ccb9944c69587",
      "f3771f27630f4b68a31b2b63d8d656c8",
      "c3d134bdc08248abb381a013e224c413",
      "67bc14e10d004fbe9c108c26bc56d108",
      "7c3b63b9276e49f7b205b292c5a7383a",
      "126585c848504c10be601c860106ae87",
      "b53e2fa09cb74eacb22ca696799f86c0",
      "0b78dbba52784f5a8b23e47bd71035b6",
      "3d3cb642ba5347cca6039b0f8d20c681",
      "c7374f8775294b51b24f5a0b895d5359",
      "35e7002ea1be4a15aa3c078cb4c6dc6e",
      "2816b513e6cf436d95db9e626bc63835",
      "f6b0ae37a5084dfb84f803becd137e1f",
      "b84aaba31c89493f91f02479c2003680",
      "0aed887123014d93ae8224d4544d9400",
      "e46e8d871de2479e9d3ae087a5a7a1e8",
      "5b6601406aba430aa3a7618480674660",
      "b4e9fcbfab4c46568ec76f76593e4305",
      "2f160b94180649cbad4b3b82a67933d8",
      "93dc0eacc41840c88701f597255dd85e",
      "9d4090abd3bb4d949e3aa0bfef133ed5",
      "21b0bd0ea24240faa750392a3070172b",
      "f3493411b37e4dd5ace09433a394acb5",
      "2918a9bed6d04353885f7fdaf3f4400d",
      "6daa9d4e2b1a4c2fa072727c95e19035",
      "90af5cbe980e46f9b34862a8fc592ede",
      "efa8470af4e048d28d627be219f4c42a",
      "95bd440484284259b0fa9c88e5019b34",
      "033a4152930548c59c76c4413eb19b99",
      "5916f44bd1734a50a864fe030f3dc66f",
      "da60036e629a449da29baa8d8aaa71a4",
      "b0c441742a954b098892a139f0f76d25",
      "9f7c45d0c8ff4af48365b8d62b3975ce",
      "cb35cb944146497ea24a182ebbbe42ab",
      "a7a8aeacc3d54c6b87703d1d5f47da68",
      "e11c0bccc1204334877576a5267214e7",
      "e25870b7ce414bec99d3c20feb8863cf",
      "2858945cec0d4c5890503bdefb7c88b1",
      "e5c833ca9e0245639a99fb8cdf6565bb",
      "04024f0c846842eb837fdae5d8d97257",
      "bb1b9c26be394108ac44dd2181552d49",
      "d4b2c7eb020047bf87d17107c7a60020",
      "1a3b94a603614897b0c6136a981d402e",
      "8a974545cdfb4790b8e53c41904a49b3",
      "7e24e869a46c444c8cca272a052818d9",
      "5e6646e9439442d2a9fe7036cdacf8c6",
      "c301834d443749e28f172298fc1c84c8",
      "c3656740c10c4ccba0676c05eed9b26f",
      "5c866f3dbc61433fb43931e14c9399f7",
      "087fcd89647f4762850d6f3d136aba4b",
      "c325bda3bbf14b84af64ee8875939ec6",
      "1056c4527c834803a55a7ace62b9f0f8",
      "3bf3899a15424a2881cfd9cef90332bf",
      "1c069617426a4b76ac2f5a8e89e8a88e",
      "eaf2ce10959c4d7fa389b78b22b6b89e",
      "88f75fff0dc44f349931c02a52512cc8",
      "bd46b02618534440af5994ab6d48b1d1",
      "99cfed60442f4740bafb3a260481089c",
      "27c837940ca94c43b3307381bf67d65e",
      "8c6d5adb2d0c44c0a94903bdd414c3e6",
      "4aed045a03c04a3cbba14bb5cf20c4f0",
      "f278bf7929da4ccea36952d37c588389",
      "e808c341e17e4f96a3026bc1304ee9b4",
      "5a7ba46ee4cb4e929b5cf844e413165b",
      "42e63b65c9e640018fa75aee0f17cdcc",
      "0972002215e945b8858493db4874e946",
      "0534ae754add46ecae2193b866c2f45b",
      "91cc09de17e945a491b17a6169d926c4",
      "5d96f887c5554787ad58fdae394b00f4",
      "a77ed2026fe64ef9b8e2ca623d2a9313",
      "e49dc82cca6b434fa7068815c8bc0165",
      "e966de4465d0416ca6d890c934f3f515",
      "c26e52fd38054de6b22343f7bdd8b09d",
      "37d5e9ef973042738f34af0fc54bd242",
      "99c7ce1a3ed4422f8a2c999f260bc87c",
      "42aee29f80c04739becc563926374edf",
      "e265307d642740eb81b45c92f6a99cd6",
      "2cbd2112ed794d479505cafc75714cae",
      "1b845d27b9aa45cf97005e052654df8e",
      "91ecfded3c9544ee9acc0b2fca49987b",
      "a2ec54a1ce404c94b86702e85bfa1467",
      "e7296efd9fe946aa875943a9d6dc8275",
      "333e3c6c7c354063b7f22fd0420c9822",
      "03f68118426449648a987a1f3cc7937f",
      "2d8ec9a74f774e7c9b7fa4b0fb2ca724",
      "85671b976b7f40439654d4e8deeacccc",
      "94d5d740914b40869efed82751dce31a",
      "85bbd994952f47aabcb56d68bc5df5f3",
      "fea709a6c071472e9db2604631196da7",
      "8a5796c9c7d7411b9994b66fea11ccbd"
     ]
    },
    "id": "f0fb5ab4",
    "outputId": "eb344dd2-e939-450d-e40f-53f6f93d9436"
   },
   "outputs": [],
   "source": [
    "# 2. Load Base Model & Tokenizer\n",
    "print(f\"Loading base model: {model_name}...\")\n",
    "print(\"This may take a few minutes (downloading ~14GB)...\")\n",
    "print()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"\\n\u2713 Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac37c11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cac37c11",
    "outputId": "e46cb03e-8928-496a-ae2e-e1bce6d94faf"
   },
   "outputs": [],
   "source": [
    "# 3. LoRA Configuration\n",
    "print(\"Configuring LoRA...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\n\u2713 LoRA adapters applied!\")\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d2956",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "4cf56e8cbaa74a4ab7d98d52d09339fa",
      "3baa62f864bd4e36a3baa3cbe067aa55",
      "091c10123b204560938554198c104879",
      "462bf862803548e889aee8328a2a8c86",
      "cd4f4a896a77417b8939e69a4205778d",
      "a8a4a43a7c57456da1f83f7a32214a47",
      "40d9368dffa44c71814515629fe5f762",
      "e4e3068666794bcdb3043d6be2a98b19",
      "d855f783178d4b268fff49b3bcde64e6",
      "e07ffb23d0a1488b8bdeea0436176cf8",
      "0eefe4cae7364d80beb74ecaaf408e1d"
     ]
    },
    "id": "839d2956",
    "outputId": "215879f4-f020-4a16-c404-12bf83d275ce"
   },
   "outputs": [],
   "source": [
    "# 4. Load Dataset\n",
    "print(\"Loading training dataset...\")\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "print(f\"\u2713 Dataset loaded: {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678552f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a678552f",
    "outputId": "874f1f70-80a5-42e4-ee61-c72cfb4dde64"
   },
   "outputs": [],
   "source": [
    "# 5. Configure Training Arguments\n",
    "print(\"Configuring training arguments...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for Colab\n",
    ")\n",
    "\n",
    "print(\"\u2713 Training arguments configured\")\n",
    "print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09a0af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165,
     "referenced_widgets": [
      "69045886ea964d7fa430e7b8f6823b4c",
      "afb6276603f4421cb0aa99345568cf9e",
      "2593880e4f3446529d5c623dab403a10",
      "6ac5ff211f0a443e8591e8d1b657664f",
      "063263aee50f4ac9a5789cfe75ec0895",
      "508efc5c8393495c933d42d3ddcc1aa8",
      "18f7292a974f4146a7884d466d794571",
      "8976a4b8b4704a87aef244e41471aa7a",
      "e2d8bd8f132f4ebc91268622c2c35cb2",
      "aa7b6a4ca1074db49e34ec591137fe21",
      "2e5b6040b51346a1bd0cf374a0e140d8",
      "8797b55d21ec4a81a97450563df06736",
      "289faa9ca6ef46c688c31a80d869d3f9",
      "13878432b32d40749f5e84ae0aa7d007",
      "321bef6ca3544a55a71521d037a3f43b",
      "dc9e8bb4beb94364a62443d32d21bc1d",
      "54f8799aa96641d0a2b36c750e211a06",
      "6e95d5abddad4fafa94983528328a2ed",
      "32924cf126204dad939f98c807b04693",
      "22238c93438e40c98abd3dc57b4db0cb",
      "fa38eed479784dd695538c12496833b9",
      "b6b9ce74ec91472eb47c8098fa0cdd74",
      "865e8a9c78c4411dac22d313298588d2",
      "29492c0650c24908b6ac8c26b4d55cfb",
      "5f21fa35fb844ca7947fa52863142856",
      "93a6bfdaf4ff45fcafab2e4effb41ded",
      "89c2a863dc924dd08d77b7a78ff0acba",
      "096ed67209304d63bc2e0419c348daa2",
      "18f3506e7ba34743941070fac4b75222",
      "b8fd07a55ab848c48706e959dcdf698d",
      "e9e4e6aa504b4764b227b6de114e9e48",
      "c7c306b65564493592440a2155502461",
      "4a0fb22805d3408ea83a90bb357147fa"
     ]
    },
    "id": "8c09a0af",
    "outputId": "8152fdeb-c228-46f6-a46b-429d2d74487a"
   },
   "outputs": [],
   "source": [
    "# 6. Initialize Trainer\n",
    "print(\"Initializing trainer...\")\n",
    "\n",
    "# Check TRL version to understand the API\n",
    "import trl\n",
    "print(f\"TRL version: {trl.__version__}\")\n",
    "\n",
    "# For newer versions of TRL, use minimal configuration\n",
    "# The model already has PEFT applied, tokenizer is inferred from model\n",
    "\n",
    "try:\n",
    "    # Try newer API first (TRL >= 0.7.0)\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=training_args,\n",
    "    )\n",
    "except TypeError as e:\n",
    "    print(f\"Trying alternative API due to: {e}\")\n",
    "    # Fall back to older API\n",
    "    from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    from transformers import Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "print(\"\u2713 Trainer initialized and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b700b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "7d8b700b",
    "outputId": "5f098c65-1ed0-443e-b710-ff8540499d3b"
   },
   "outputs": [],
   "source": [
    "# 7. Start Training\n",
    "print(\"=\"*60)\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This will take some time depending on your GPU and dataset size.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2713 Training complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2b707",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fa2b707",
    "outputId": "8ab021f7-9455-486f-c5e9-7a3ed1d2b975"
   },
   "outputs": [],
   "source": [
    "# 8. Save the Adapter\n",
    "print(f\"Saving adapter to {adapter_name}...\")\n",
    "\n",
    "trainer.model.save_pretrained(adapter_name)\n",
    "tokenizer.save_pretrained(adapter_name)\n",
    "\n",
    "print(\"\u2713 Adapter saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b39ba",
   "metadata": {
    "id": "d32b39ba"
   },
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "Let's compare the base model with our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e1784",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "9b1210c6e88c4edb9f4a45062b9241fb",
      "974719710081430580888773d2b6c9eb",
      "f1271554695e46188f6c3329a119806b",
      "153bce81e4be4ef4846e734292a39345",
      "9ef639d0ac5a49c680dc9843640e312d",
      "13b4c8388c7a4cb6a6e5a05b0cd8badd",
      "8261629cdbcd45dcb03aea5438367814",
      "fbafa21d5b464f78980d27b13004c264",
      "5cb0c2d9a61540ebbd86fd8c8765bdc1",
      "462f973da2ce411aa26ece4c1324e5ab",
      "be793a890819466c9f684e39b760389a"
     ]
    },
    "id": "ca7e1784",
    "outputId": "2c15a78f-ca5c-4f14-c424-83e4ead9e745"
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "print(\"Loading fine-tuned model for evaluation...\")\n",
    "\n",
    "base_model_eval = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tuned_model = PeftModel.from_pretrained(base_model_eval, adapter_name)\n",
    "tuned_model = tuned_model.merge_and_unload()\n",
    "\n",
    "print(\"\u2713 Fine-tuned model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44788ec8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44788ec8",
    "outputId": "834afb0f-9d20-49b1-c8ea-1d806364bef7"
   },
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"<s> Analyze the following: 'A new startup, InnovateAI, just raised a $50M Series A round, but has no revenue.'\"\n",
    "\n",
    "print(\"Test Prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf80e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bccf80e5",
    "outputId": "6b176d93-5865-4d60-b491-4cf0e020e651"
   },
   "outputs": [],
   "source": [
    "# Base Model Response\n",
    "print(\"\\n--- BASE MODEL (Gemma 7B IT) ---\\n\")\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model_eval.generate(**inputs, max_new_tokens=150, temperature=0.7)\n",
    "base_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(base_response)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b473b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2b473b7",
    "outputId": "994b36f4-e191-4d93-ec82-6ad7374822d5"
   },
   "outputs": [],
   "source": [
    "# Fine-tuned Model Response\n",
    "print(\"\\n--- FINE-TUNED MODEL (Financial Analyst) ---\\n\")\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = tuned_model.generate(**inputs, max_new_tokens=150, temperature=0.1)\n",
    "tuned_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(tuned_response)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57da077",
   "metadata": {
    "id": "e57da077"
   },
   "source": [
    "## 7. Save Adapter to Cloud Storage\n",
    "\n",
    "Upload the fine-tuned LoRA adapter to Google Cloud Storage for safe keeping and future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23b7d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc23b7d4",
    "outputId": "458bf90d-b378-4e4d-b83b-6fbda8c5dd4f"
   },
   "outputs": [],
   "source": [
    "# Create a Cloud Storage bucket (if needed)\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-gemma-finetuning\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "print(f\"Bucket: {BUCKET_URI}\")\n",
    "\n",
    "# Create bucket\n",
    "!gsutil mb -l {REGION} {BUCKET_URI} 2>/dev/null || echo \"Bucket already exists or error creating bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403c73a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9403c73a",
    "outputId": "f6e90ba9-d747-465a-8a12-14368821ac9f"
   },
   "outputs": [],
   "source": [
    "# Upload adapter to Cloud Storage\n",
    "MODEL_GCS_PATH = f\"{BUCKET_URI}/models/{adapter_name}/\"\n",
    "\n",
    "print(f\"Uploading adapter to {MODEL_GCS_PATH}...\")\n",
    "!gsutil -m cp -r {adapter_name}/* {MODEL_GCS_PATH}\n",
    "\n",
    "print(\"\u2713 Adapter uploaded to Cloud Storage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e2c1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "020e2c1a",
    "outputId": "08ff413f-2b82-4eb9-8280-8531b98d5ed6"
   },
   "outputs": [],
   "source": [
    "# Verify upload and display adapter location\n",
    "print(\"=\"*70)\n",
    "print(\"\u2713 LoRA Adapter Successfully Saved!\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"\ud83d\udce6 Adapter Location: {MODEL_GCS_PATH}\")\n",
    "print()\n",
    "print(\"Your fine-tuned adapter is now stored in Cloud Storage.\")\n",
    "print(\"You can:\")\n",
    "print(\"  1. Download it locally (see next section)\")\n",
    "print(\"  2. Load it from GCS in other notebooks or applications\")\n",
    "print(\"  3. Share the GCS path with your team\")\n",
    "print()\n",
    "print(\"Example - Load adapter from GCS:\")\n",
    "print(f\"  adapter = PeftModel.from_pretrained(base_model, '{MODEL_GCS_PATH}')\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36f0d4",
   "metadata": {
    "id": "dc36f0d4"
   },
   "source": [
    "### 7.1. Prepare for Vertex AI Deployment\n",
    "\n",
    "To deploy to Vertex AI Model Registry and Endpoints, we need to merge the adapter with the base model and prepare it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c023492",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327,
     "referenced_widgets": [
      "1be3e59d910d498f8e031b972b46007c",
      "a7dc9c84eb064e7f948cc9044cfc087e",
      "1849f140a207406998b40f8914faab24",
      "31171f6478dc488e8c925b027f5cff9e",
      "18144714952b485293757d07f611046a",
      "b2a70147511c4293a50c56febb60044f",
      "0ffd79beb4f54abc865268943b72d444",
      "65d3e77bc2f94909bfd86f10f173bb75",
      "e82c0f51f1dc4f42b0e02e8b40c8ae6d",
      "aa3931c8d32d48c5ad7de09ad2cf5283",
      "85ddd03d2008447ba4671b0d54e3087a"
     ]
    },
    "id": "2c023492",
    "outputId": "0a360f32-2098-4a34-cebc-9656f2d03332"
   },
   "outputs": [],
   "source": [
    "# Merge adapter with base model for deployment\n",
    "print(\"Merging LoRA adapter with base model...\")\n",
    "print(\"This creates a single model ready for deployment\")\n",
    "print()\n",
    "\n",
    "# Free up GPU memory first - clear ALL models from previous sections\n",
    "print(\"Clearing GPU memory...\")\n",
    "import gc\n",
    "\n",
    "# Delete all possible model references\n",
    "try:\n",
    "    del model  # Training model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del base_model_eval  # Evaluation base model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del tuned_model  # Evaluation fine-tuned model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del deployment_model  # In case this cell was run before\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Force garbage collection and clear CUDA cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Display GPU memory status\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\u2713 GPU memory cleared\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Load base model WITH quantization (same as training)\n",
    "# This is necessary because the adapter was trained on a quantized model\n",
    "print(\"Loading base model for deployment...\")\n",
    "deployment_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # Use same quantization as training\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load and merge the adapter\n",
    "print(\"Loading and merging adapter...\")\n",
    "deployment_model = PeftModel.from_pretrained(deployment_model, adapter_name)\n",
    "deployment_model = deployment_model.merge_and_unload()\n",
    "\n",
    "print(\"\u2713 Adapter merged with base model!\")\n",
    "print()\n",
    "\n",
    "# Save merged model for deployment\n",
    "merged_model_dir = \"./gemma-7b-analyst-merged\"\n",
    "print(f\"Saving merged model to {merged_model_dir}...\")\n",
    "deployment_model.save_pretrained(merged_model_dir)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "\n",
    "print(f\"\u2713 Merged model saved to: {merged_model_dir}\")\n",
    "print()\n",
    "print(\"Note: Model is saved in 4-bit quantized format for efficient deployment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856b516",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f856b516",
    "outputId": "89b23bde-34b6-4280-8dba-1b2248f395a4"
   },
   "outputs": [],
   "source": [
    "# Upload merged model to Cloud Storage\n",
    "MERGED_MODEL_GCS_PATH = f\"{BUCKET_URI}/models/gemma-7b-analyst-merged/\"\n",
    "\n",
    "print(f\"Uploading merged model to {MERGED_MODEL_GCS_PATH}...\")\n",
    "print(\"This may take 5-10 minutes (uploading ~14GB)...\")\n",
    "!gsutil -m cp -r {merged_model_dir}/* {MERGED_MODEL_GCS_PATH}\n",
    "\n",
    "print(\"\\n\u2713 Merged model uploaded to Cloud Storage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39948a",
   "metadata": {
    "id": "ae39948a"
   },
   "source": [
    "### 7.2. Deployment Options\n",
    "\n",
    "For deploying HuggingFace models on Vertex AI, you have several options:\n",
    "\n",
    "**Option A: Local/Colab Inference** (Recommended for testing)\n",
    "- Load the merged model directly in notebooks\n",
    "- Use for development and testing\n",
    "- No deployment costs\n",
    "\n",
    "**Option B: Vertex AI Prediction with Custom Container** (Production)\n",
    "- Create a Docker container with transformers library\n",
    "- Deploy to Vertex AI Endpoints\n",
    "- Requires additional setup (see cells below)\n",
    "\n",
    "**Option C: Cloud Run** (Serverless)\n",
    "- Wrap model in FastAPI\n",
    "- Deploy to Cloud Run\n",
    "- Auto-scales to zero when not in use\n",
    "\n",
    "We'll demonstrate Option A below. For production (Option B/C), see the notes at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45030d35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45030d35",
    "outputId": "7d15acaf-67ac-4d09-b094-e5359347146d"
   },
   "outputs": [],
   "source": [
    "# Option A: Load Model for Inference (Recommended)\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL DEPLOYMENT - OPTION A: Direct Inference\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Your fine-tuned model is saved and ready to use!\")\n",
    "print()\n",
    "print(f\"\ud83d\udce6 Model Location: {MERGED_MODEL_GCS_PATH}\")\n",
    "print()\n",
    "print(\"To use the model for inference:\")\n",
    "print()\n",
    "print(\"# Load from Cloud Storage\")\n",
    "print(\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
    "print(\"import torch\")\n",
    "print()\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{MERGED_MODEL_GCS_PATH}', torch_dtype=torch.float16, device_map='auto')\")\n",
    "print(f\"tokenizer = AutoTokenizer.from_pretrained('{MERGED_MODEL_GCS_PATH}')\")\n",
    "print()\n",
    "print(\"# Or load from local directory\")\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{merged_model_dir}', torch_dtype=torch.float16, device_map='auto')\")\n",
    "print(f\"tokenizer = AutoTokenizer.from_pretrained('{merged_model_dir}')\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"\u2713 Model is ready for use in notebooks, scripts, or applications!\")\n",
    "print()\n",
    "print(\"See the cells below for inference examples and production deployment options.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98386d93",
   "metadata": {
    "id": "98386d93"
   },
   "source": [
    "### 7.3. Test Your Fine-Tuned Model\n",
    "\n",
    "Let's test the fine-tuned model with some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d44a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c05d44a0",
    "outputId": "e96de243-2ea8-418c-81e6-de48ba594cb6"
   },
   "outputs": [],
   "source": [
    "# Test the fine-tuned model with inference\n",
    "print(\"Testing fine-tuned model...\")\n",
    "print()\n",
    "\n",
    "# The deployment_model is already loaded and merged from the previous section\n",
    "# If you need to reload it, uncomment below:\n",
    "# deployment_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     merged_model_dir,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(merged_model_dir)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Analyze the following: 'Company XYZ reported Q4 revenue of $2B, up 25% YoY, exceeding analyst expectations.'\",\n",
    "    \"Analyze the following: 'Startup ABC raised $100M Series B but laid off 20% of staff.'\",\n",
    "    \"Analyze the following: 'TechCorp stock dropped 15% after missing earnings targets by $50M.'\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINE-TUNED MODEL PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print()\n",
    "\n",
    "    # Format with instruction template\n",
    "    full_prompt = f\"<s> {prompt}\"\n",
    "\n",
    "    # Generate response\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(deployment_model.device)\n",
    "    outputs = deployment_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract just the model's response (after the prompt)\n",
    "    response = response[len(full_prompt):].strip()\n",
    "\n",
    "    print(f\"Response: {response}\")\n",
    "    print()\n",
    "    print(\"-\"*70)\n",
    "    print()\n",
    "\n",
    "print(\"\u2713 Model inference complete!\")\n",
    "print()\n",
    "print(f\"Your model is saved at: {MERGED_MODEL_GCS_PATH}\")\n",
    "print(f\"Local copy at: {merged_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965156aa",
   "metadata": {
    "id": "965156aa"
   },
   "source": [
    "### 7.4. Production Deployment Options\n",
    "\n",
    "For production deployment on GCP, here are your options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94473c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b94473c1",
    "outputId": "fc368453-3ff1-44fd-b77c-85953ef9cc7e"
   },
   "outputs": [],
   "source": [
    "# Production Deployment Guide\n",
    "print(\"=\"*70)\n",
    "print(\"PRODUCTION DEPLOYMENT OPTIONS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"Your fine-tuned model is saved and ready for production!\")\n",
    "print(f\"Model Location: {MERGED_MODEL_GCS_PATH}\")\n",
    "print()\n",
    "\n",
    "print(\"OPTION 1: Vertex AI Workbench (Easiest)\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\u2022 Create a Vertex AI Workbench notebook instance\")\n",
    "print(\"\u2022 Load model from GCS and serve predictions\")\n",
    "print(\"\u2022 Good for: Internal tools, batch processing\")\n",
    "print(\"\u2022 Cost: ~$0.20/hour for notebook instance\")\n",
    "print()\n",
    "\n",
    "print(\"OPTION 2: Cloud Run (Serverless)\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\u2022 Create a FastAPI wrapper around your model\")\n",
    "print(\"\u2022 Deploy to Cloud Run with GPU support\")\n",
    "print(\"\u2022 Good for: Variable traffic, cost optimization\")\n",
    "print(\"\u2022 Cost: Pay only when serving requests\")\n",
    "print()\n",
    "print(\"Example FastAPI app:\")\n",
    "print(\"\"\"\n",
    "from fastapi import FastAPI\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'gs://YOUR-BUCKET/models/gemma-7b-analyst-merged/',\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('gs://YOUR-BUCKET/models/gemma-7b-analyst-merged/')\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(prompt: str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "    return {\"response\": tokenizer.decode(outputs[0], skip_special_tokens=True)}\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "print(\"OPTION 3: Vertex AI Prediction (Custom Container)\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\u2022 Build custom Docker container with transformers\")\n",
    "print(\"\u2022 Deploy to Vertex AI Prediction Endpoints\")\n",
    "print(\"\u2022 Good for: Enterprise, SLA requirements\")\n",
    "print(\"\u2022 Cost: ~$0.50-$1.00/hour for T4 GPU\")\n",
    "print()\n",
    "print(\"Required files:\")\n",
    "print(\"  - Dockerfile (with transformers, torch, fastapi)\")\n",
    "print(\"  - predictor.py (custom prediction handler)\")\n",
    "print(\"  - requirements.txt\")\n",
    "print()\n",
    "\n",
    "print(\"OPTION 4: Vertex AI Model Garden Deploy (Coming Soon)\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\u2022 Upload to Model Garden for one-click deployment\")\n",
    "print(\"\u2022 Requires TorchServe .mar archive format\")\n",
    "print(\"\u2022 Good for: Standardized deployments\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"RECOMMENDED: Start with Vertex AI Workbench for testing,\")\n",
    "print(\"then move to Cloud Run or Vertex AI Prediction for production.\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d2189",
   "metadata": {
    "id": "3e2d2189"
   },
   "source": [
    "### 7.5. Using Your Model from Other Notebooks\n",
    "\n",
    "Load your fine-tuned model from anywhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee943bf6",
   "metadata": {
    "id": "ee943bf6"
   },
   "outputs": [],
   "source": [
    "# Example: Load and use your model from another notebook or script\n",
    "print(\"=\"*70)\n",
    "print(\"USING YOUR MODEL FROM OTHER NOTEBOOKS/SCRIPTS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "code_example = f'''\n",
    "# Install required packages\n",
    "!pip install transformers torch accelerate\n",
    "\n",
    "# Import libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model from Cloud Storage\n",
    "model_path = \"{MERGED_MODEL_GCS_PATH}\"\n",
    "\n",
    "print(\"Loading model from GCS...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Make a prediction\n",
    "def analyze_financial_news(text):\n",
    "    prompt = f\"<s> Analyze the following: '{{text}}'\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "result = analyze_financial_news(\"Company ABC reported record Q4 earnings of $5B.\")\n",
    "print(result)\n",
    "'''\n",
    "\n",
    "print(code_example)\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"Model GCS Path: {MERGED_MODEL_GCS_PATH}\")\n",
    "print(f\"Model Local Path: {merged_model_dir}\")\n",
    "print()\n",
    "print(\"\u2713 Copy the code above to use your model anywhere!\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ec2f3",
   "metadata": {
    "id": "3f6ec2f3"
   },
   "outputs": [],
   "source": [
    "# Clean up GPU memory when done\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"To free up GPU memory when you're done:\")\n",
    "print()\n",
    "\n",
    "cleanup_code = '''\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Delete model from memory\n",
    "del deployment_model\n",
    "del tokenizer\n",
    "\n",
    "# Clear GPU cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\u2713 GPU memory cleared!\")\n",
    "'''\n",
    "\n",
    "print(cleanup_code)\n",
    "print()\n",
    "print(\"Note: Your model files are safely stored in Cloud Storage\")\n",
    "print(f\"Location: {MERGED_MODEL_GCS_PATH}\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e9864",
   "metadata": {
    "id": "c70e9864"
   },
   "source": [
    "## 8. Download Model (Optional)\n",
    "\n",
    "Download the fine-tuned adapter to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fec38e",
   "metadata": {
    "id": "75fec38e"
   },
   "outputs": [],
   "source": [
    "# Download the adapter\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "    # Create a zip file\n",
    "    shutil.make_archive(adapter_name, 'zip', adapter_name)\n",
    "\n",
    "    # Download\n",
    "    from google.colab import files\n",
    "    files.download(f\"{adapter_name}.zip\")\n",
    "\n",
    "    print(f\"\u2713 Downloaded {adapter_name}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c172e4f",
   "metadata": {
    "id": "9c172e4f"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You have successfully:\n",
    "\n",
    "1. \u2713 Authenticated with Google Cloud and HuggingFace\n",
    "2. \u2713 Loaded Gemma 7B model from HuggingFace\n",
    "3. \u2713 Configured QLoRA (4-bit quantization + LoRA adapters)\n",
    "4. \u2713 Fine-tuned Gemma on your financial analysis dataset\n",
    "5. \u2713 Evaluated the model's performance\n",
    "6. \u2713 Merged adapter with base model for deployment\n",
    "7. \u2713 Uploaded model to Google Cloud Storage\n",
    "8. \u2713 Tested the fine-tuned model with predictions\n",
    "\n",
    "**Your Fine-Tuned Model:**\n",
    "- **Model Name:** gemma-7b-financial-analyst\n",
    "- **GCS Location:** `gs://{PROJECT_ID}-gemma-finetuning/models/gemma-7b-analyst-merged/`\n",
    "- **Local Location:** `./gemma-7b-analyst-merged/`\n",
    "- **Format:** HuggingFace Transformers (4-bit quantized)\n",
    "\n",
    "**Using Your Model:**\n",
    "\n",
    "From any Python environment:\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load from Cloud Storage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gs://{PROJECT_ID}-gemma-finetuning/models/gemma-7b-analyst-merged/\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"gs://{PROJECT_ID}-gemma-finetuning/models/gemma-7b-analyst-merged/\"\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "prompt = \"Analyze the following: 'Your financial news here'\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "**Production Deployment Options:**\n",
    "\n",
    "1. **Vertex AI Workbench** (Recommended for testing)\n",
    "   - Create notebook instance\n",
    "   - Load model and serve predictions\n",
    "   - Cost: ~$0.20/hour\n",
    "\n",
    "2. **Cloud Run** (Serverless, cost-effective)\n",
    "   - Wrap model in FastAPI\n",
    "   - Deploy with GPU support\n",
    "   - Auto-scales to zero\n",
    "   - Cost: Pay per request\n",
    "\n",
    "3. **Vertex AI Prediction** (Enterprise)\n",
    "   - Custom Docker container\n",
    "   - Managed endpoints with SLAs\n",
    "   - Cost: ~$0.50-$1.00/hour (T4 GPU)\n",
    "\n",
    "**Storage Costs:**\n",
    "- Model in GCS: ~$0.02/month (4GB quantized model)\n",
    "- Adapter only: ~$0.001/month (small adapter files)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. **Test thoroughly** with your use cases\n",
    "2. **Choose deployment method** based on your needs:\n",
    "   - Low traffic \u2192 Vertex AI Workbench\n",
    "   - Variable traffic \u2192 Cloud Run\n",
    "   - High traffic/SLA \u2192 Vertex AI Prediction\n",
    "3. **Set up monitoring** with Cloud Logging\n",
    "4. **Implement CI/CD** for model updates\n",
    "5. **Consider** training on larger datasets for production\n",
    "\n",
    "**Resources Created:**\n",
    "- LoRA Adapter: `gs://{PROJECT_ID}-gemma-finetuning/models/gemma-7b-analyst-adapter/`\n",
    "- Merged Model: `gs://{PROJECT_ID}-gemma-finetuning/models/gemma-7b-analyst-merged/`\n",
    "- Dataset: `dataset.jsonl` (102 examples)\n",
    "\n",
    "**Need Help?**\n",
    "- HuggingFace Transformers docs: https://huggingface.co/docs/transformers\n",
    "- Vertex AI docs: https://cloud.google.com/vertex-ai/docs\n",
    "- Cloud Run GPU: https://cloud.google.com/run/docs/configuring/services/gpu\n",
    "\n",
    "\ud83c\udf89 **Your fine-tuned Gemma model is ready for production use!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}