{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f8eb57",
   "metadata": {},
   "source": [
    "# Chapter 6: Deploy Models from Vertex AI Model Garden\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ayoisio/genai-on-google-cloud/blob/main/chapter-6/colabs/02_model_garden_deployment.ipynb)\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "- Navigate and select models from Vertex AI Model Garden\n",
    "- Deploy open-source models using vLLM serving containers\n",
    "- Configure hardware based on model size requirements\n",
    "- Send inference requests to deployed endpoints\n",
    "- Manage costs by cleaning up resources\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A Google Cloud project with Vertex AI API enabled\n",
    "- Familiarity with Python and the Google Cloud SDK\n",
    "- (Optional) A HuggingFace account for gated model access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b29f7e",
   "metadata": {},
   "source": [
    "## 1. Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb99c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    print(\"✓ Authentication successful!\")\n",
    "else:\n",
    "    print(\"Running outside Colab - ensure gcloud is configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-cloud-aiplatform>=1.50.0\n",
    "print(\"✓ Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure project settings\n",
    "import os\n",
    "\n",
    "# Set your project details\n",
    "PROJECT_ID = input(\"Enter your GCP Project ID: \")\n",
    "REGION = input(\"Enter your region (e.g., us-central1): \") or \"us-central1\"\n",
    "\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
    "\n",
    "print(f\"\\n✓ Project: {PROJECT_ID}\")\n",
    "print(f\"✓ Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c177ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(f\"✓ Vertex AI initialized\")\n",
    "print(f\"  Project: {PROJECT_ID}\")\n",
    "print(f\"  Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e9953",
   "metadata": {},
   "source": [
    "## 2. Understanding Model Garden\n",
    "\n",
    "Vertex AI Model Garden provides access to:\n",
    "- **First-party models**: Gemini, Gemma, PaLM (Google models)\n",
    "- **Third-party models**: Llama, Mistral, Claude (partner models)\n",
    "- **Open-source models**: Community models with various licenses\n",
    "\n",
    "### Model Selection Framework\n",
    "\n",
    "| Model Size | Use Case | GPU Requirement | Example |\n",
    "|------------|----------|-----------------|----------|\n",
    "| 2B-4B | Edge/Mobile, Low latency | 1x L4 (24GB) | Gemma 2B |\n",
    "| 7B-9B | General purpose, Balance | 1x L4 or A100 | Gemma 7B, Llama 3 8B |\n",
    "| 27B-70B | Complex reasoning | 2-4x A100 (80GB) | Gemma 27B, Llama 3 70B |\n",
    "| 400B+ | Research, Max capability | TPU pods or 8x H100 | Llama 3 405B |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41e1b0",
   "metadata": {},
   "source": [
    "## 3. Deploy Gemma from Model Garden\n",
    "\n",
    "We'll deploy Gemma 2 using the vLLM serving container for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for deployment\n",
    "import datetime\n",
    "\n",
    "# Model selection\n",
    "MODEL_ID = \"gemma-2-2b-it\"  # Options: gemma-2-2b-it, gemma-2-9b-it, gemma-2-27b-it\n",
    "HF_MODEL_ID = f\"google/{MODEL_ID}\"\n",
    "\n",
    "# vLLM serving container\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250116_0916_RC00\"\n",
    "\n",
    "# Hardware configuration based on model size\n",
    "if \"2b\" in MODEL_ID:\n",
    "    MACHINE_TYPE = \"g2-standard-8\"\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_L4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "    GPU_MEMORY_UTILIZATION = 0.9\n",
    "    MAX_MODEL_LEN = 8192\n",
    "elif \"9b\" in MODEL_ID:\n",
    "    MACHINE_TYPE = \"g2-standard-24\"\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_L4\"\n",
    "    ACCELERATOR_COUNT = 2\n",
    "    GPU_MEMORY_UTILIZATION = 0.9\n",
    "    MAX_MODEL_LEN = 8192\n",
    "else:  # 27b\n",
    "    MACHINE_TYPE = \"a2-highgpu-4g\"\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_A100_80GB\"\n",
    "    ACCELERATOR_COUNT = 4\n",
    "    GPU_MEMORY_UTILIZATION = 0.95\n",
    "    MAX_MODEL_LEN = 8192\n",
    "\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Machine: {MACHINE_TYPE}\")\n",
    "print(f\"Accelerator: {ACCELERATOR_COUNT}x {ACCELERATOR_TYPE}\")\n",
    "print(f\"Max context length: {MAX_MODEL_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set HuggingFace token for gated models\n",
    "# Some models require accepting license terms on HuggingFace\n",
    "\n",
    "HF_TOKEN = input(\"Enter HuggingFace token (or press Enter to skip): \") or None\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"✓ HuggingFace token configured\")\n",
    "else:\n",
    "    print(\"⚠ No HF token - using public model access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d21991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    machine_type: str,\n",
    "    accelerator_type: str,\n",
    "    accelerator_count: int,\n",
    "    gpu_memory_utilization: float = 0.9,\n",
    "    max_model_len: int = 4096,\n",
    "):\n",
    "    \"\"\"Deploy a model using vLLM serving container.\"\"\"\n",
    "    \n",
    "    # vLLM arguments\n",
    "    vllm_args = [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=8080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    \n",
    "    # Environment variables\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "    \n",
    "    if HF_TOKEN:\n",
    "        env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    \n",
    "    # Create endpoint\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name}-endpoint\",\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "    \n",
    "    # Upload model\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[8080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        model_garden_source_model_name=\"publishers/google/models/gemma2\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s)...\")\n",
    "    print(\"This may take 15-30 minutes...\")\n",
    "    \n",
    "    # Deploy to endpoint\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Deployment complete!\")\n",
    "    print(f\"  Endpoint: {endpoint.name}\")\n",
    "    \n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute deployment\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"{MODEL_ID}-{timestamp}\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEPLOYING MODEL FROM MODEL GARDEN\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=model_name,\n",
    "    model_id=HF_MODEL_ID,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00621700",
   "metadata": {},
   "source": [
    "## 4. Send Inference Requests\n",
    "\n",
    "Now let's test our deployed model with some prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a773fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for predictions\n",
    "def generate_response(endpoint, prompt, max_tokens=256, temperature=0.7):\n",
    "    \"\"\"Generate a response from the deployed model.\"\"\"\n",
    "    \n",
    "    # Format for vLLM\n",
    "    instances = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 0.9,\n",
    "    }]\n",
    "    \n",
    "    response = endpoint.predict(instances=instances)\n",
    "    \n",
    "    return response.predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f945d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain the concept of fine-tuning in machine learning in 3 sentences.\",\n",
    "    \"What are the key differences between GPUs and TPUs for AI workloads?\",\n",
    "    \"Write a Python function to calculate fibonacci numbers.\",\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL INFERENCE TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print()\n",
    "    \n",
    "    response = generate_response(endpoint, prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bedd663",
   "metadata": {},
   "source": [
    "## 5. Monitor Endpoint Performance\n",
    "\n",
    "Check the endpoint metrics and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a233c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get endpoint details\n",
    "print(\"=\"*70)\n",
    "print(\"ENDPOINT INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"Endpoint Name: {endpoint.display_name}\")\n",
    "print(f\"Endpoint Resource: {endpoint.resource_name}\")\n",
    "print(f\"Endpoint URI: {endpoint.name}\")\n",
    "print()\n",
    "\n",
    "# List deployed models\n",
    "for deployed_model in endpoint.list_models():\n",
    "    print(f\"Deployed Model ID: {deployed_model.id}\")\n",
    "    print(f\"Model Display Name: {deployed_model.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763dffb",
   "metadata": {},
   "source": [
    "## 6. Clean Up Resources\n",
    "\n",
    "**Important**: Delete resources to avoid ongoing charges. A deployed model can cost $1-5+ per hour depending on GPU type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - Uncomment to delete resources\n",
    "cleanup = input(\"Delete deployed resources? (yes/no): \").lower()\n",
    "\n",
    "if cleanup == \"yes\":\n",
    "    print(\"\\nCleaning up resources...\")\n",
    "    \n",
    "    # Undeploy model from endpoint\n",
    "    endpoint.undeploy_all()\n",
    "    print(\"✓ Model undeployed\")\n",
    "    \n",
    "    # Delete endpoint\n",
    "    endpoint.delete()\n",
    "    print(\"✓ Endpoint deleted\")\n",
    "    \n",
    "    # Delete model\n",
    "    model.delete()\n",
    "    print(\"✓ Model deleted\")\n",
    "    \n",
    "    print(\"\\n✓ All resources cleaned up!\")\n",
    "else:\n",
    "    print(\"\\n⚠ Resources retained - remember to delete them later to avoid charges!\")\n",
    "    print(f\"\\nTo delete later, run:\")\n",
    "    print(f\"  endpoint = aiplatform.Endpoint('{endpoint.resource_name}')\")\n",
    "    print(f\"  endpoint.undeploy_all()\")\n",
    "    print(f\"  endpoint.delete()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390af57",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Navigate Model Garden** - Select models based on size, capability, and hardware requirements\n",
    "2. **Deploy with vLLM** - Use optimized serving containers for efficient inference\n",
    "3. **Configure hardware** - Match GPU resources to model size requirements\n",
    "4. **Send requests** - Generate predictions from deployed endpoints\n",
    "5. **Manage costs** - Clean up resources to avoid ongoing charges\n",
    "\n",
    "### Cost Estimates\n",
    "\n",
    "| Configuration | Hourly Cost (approx) |\n",
    "|---------------|----------------------|\n",
    "| 1x L4 (g2-standard-8) | ~$0.80-1.20/hour |\n",
    "| 2x L4 (g2-standard-24) | ~$1.80-2.40/hour |\n",
    "| 4x A100-80GB | ~$12-16/hour |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try deploying different models (Llama 3, Mistral)\n",
    "- Experiment with batch inference for higher throughput\n",
    "- Explore the [Model Garden UI](https://console.cloud.google.com/vertex-ai/model-garden) for one-click deployments\n",
    "- See **03_vllm_serving.ipynb** for advanced vLLM configuration"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
