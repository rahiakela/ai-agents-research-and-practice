{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RAG Pipeline: Context Assembly\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ayoisio/genai-on-google-cloud/blob/main/chapter-2/colabs/04_rag_context_assembly.ipynb)\n\n**Estimated Time**: 15 minutes\n\n**Prerequisites**: Google Cloud project with billing enabled, Vertex AI API enabled\n\n---\n\n## Overview\n\nRetrieval-Augmented Generation (RAG) enhances LLM responses with relevant context from your data. This notebook demonstrates:\n\n1. **Build a complete RAG pipeline** from retrieval to generation\n2. **Assemble context** with source attribution\n3. **Generate grounded responses** using retrieved documents\n4. **Handle edge cases** and optimize context usage\n\nThis implements the context assembly pattern from Example 2-2 in Chapter 2.\n\n```mermaid\nflowchart LR\n    A[Query] --> B[Embed]\n    B --> C[Retrieve]\n    C --> D[Assemble Context]\n    D --> E[Generate]\n    E --> F[Answer]\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install Dependencies\n",
    "!pip install --upgrade google-cloud-aiplatform google-generativeai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Authenticate with Google Cloud\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "print(\"‚úì Authentication successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configure Your Project\n",
    "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Validate project ID\n",
    "if PROJECT_ID == \"your-project-id\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID above\")\n",
    "\n",
    "print(f\"‚úì Project: {PROJECT_ID}\")\n",
    "print(f\"‚úì Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize Vertex AI\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "import numpy as np\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Initialize models\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\n",
    "generative_model = GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "print(f\"‚úì Vertex AI initialized\")\n",
    "print(f\"‚úì Embedding model: text-embedding-005\")\n",
    "print(f\"‚úì Generative model: gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Knowledge Base\n",
    "\n",
    "First, let's create a sample knowledge base with document chunks and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define sample knowledge base\n",
    "# Simulating a knowledge base about AI and Machine Learning\n",
    "\n",
    "KNOWLEDGE_BASE = [\n",
    "    {\n",
    "        \"id\": \"ml_basics_1\",\n",
    "        \"source\": \"ML Fundamentals Guide\",\n",
    "        \"section\": \"Chapter 1: Introduction\",\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. The core idea is to develop algorithms that can access data and use it to learn for themselves.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ml_basics_2\",\n",
    "        \"source\": \"ML Fundamentals Guide\",\n",
    "        \"section\": \"Chapter 2: Types of ML\",\n",
    "        \"content\": \"There are three main types of machine learning: supervised learning (using labeled data), unsupervised learning (finding patterns without labels), and reinforcement learning (learning through rewards and penalties). Each type is suited for different problem domains.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"dl_intro_1\",\n",
    "        \"source\": \"Deep Learning Handbook\",\n",
    "        \"section\": \"Neural Networks\",\n",
    "        \"content\": \"Deep learning uses neural networks with multiple layers (hence 'deep') to progressively extract higher-level features from raw input. For example, in image recognition, lower layers identify edges, while higher layers identify concepts like faces or objects.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"dl_intro_2\",\n",
    "        \"source\": \"Deep Learning Handbook\",\n",
    "        \"section\": \"Training Process\",\n",
    "        \"content\": \"Training a neural network involves forward propagation (computing predictions), calculating loss (error), and backpropagation (adjusting weights). This process repeats over many iterations until the model converges to an optimal solution.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"nlp_basics_1\",\n",
    "        \"source\": \"NLP Reference Manual\",\n",
    "        \"section\": \"Text Processing\",\n",
    "        \"content\": \"Natural Language Processing (NLP) enables computers to understand, interpret, and generate human language. Key tasks include tokenization, part-of-speech tagging, named entity recognition, and sentiment analysis.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"llm_overview_1\",\n",
    "        \"source\": \"LLM Architecture Guide\",\n",
    "        \"section\": \"Transformer Models\",\n",
    "        \"content\": \"Large Language Models (LLMs) are based on the Transformer architecture, which uses self-attention mechanisms to process input sequences. This allows the model to weigh the importance of different parts of the input when generating output.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"llm_overview_2\",\n",
    "        \"source\": \"LLM Architecture Guide\",\n",
    "        \"section\": \"Training and Fine-tuning\",\n",
    "        \"content\": \"LLMs are typically pre-trained on vast amounts of text data, then fine-tuned for specific tasks. Pre-training gives the model general language understanding, while fine-tuning adapts it to domain-specific requirements.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rag_intro_1\",\n",
    "        \"source\": \"RAG Implementation Guide\",\n",
    "        \"section\": \"Overview\",\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) combines the power of LLMs with external knowledge retrieval. Instead of relying solely on the model's trained knowledge, RAG retrieves relevant documents and uses them to generate more accurate, up-to-date responses.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rag_intro_2\",\n",
    "        \"source\": \"RAG Implementation Guide\",\n",
    "        \"section\": \"Benefits\",\n",
    "        \"content\": \"Key benefits of RAG include: reduced hallucinations (grounding in real data), ability to cite sources, easy knowledge updates without retraining, and domain-specific accuracy. RAG is particularly valuable for enterprise applications.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"vector_search_1\",\n",
    "        \"source\": \"Vector Database Guide\",\n",
    "        \"section\": \"Semantic Search\",\n",
    "        \"content\": \"Vector search enables semantic similarity matching by converting text to embeddings (dense numerical vectors). Unlike keyword search, vector search finds conceptually similar content even when exact words differ.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úì Created knowledge base with {len(KNOWLEDGE_BASE)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate embeddings for knowledge base\n",
    "# Get content from all documents\n",
    "contents = [doc['content'] for doc in KNOWLEDGE_BASE]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_model.get_embeddings(contents)\n",
    "\n",
    "# Add embeddings to documents\n",
    "for doc, emb in zip(KNOWLEDGE_BASE, embeddings):\n",
    "    doc['embedding'] = np.array(emb.values)\n",
    "\n",
    "print(f\"‚úì Generated embeddings for {len(KNOWLEDGE_BASE)} documents\")\n",
    "print(f\"  Embedding dimension: {len(KNOWLEDGE_BASE[0]['embedding'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieval Component\n",
    "\n",
    "The retrieval component finds relevant documents based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Retrieval function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def retrieve_documents(query, knowledge_base, top_k=3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from the knowledge base.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        knowledge_base: List of documents with embeddings\n",
    "        top_k: Number of documents to retrieve\n",
    "        threshold: Minimum similarity score\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents with similarity scores\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "    query_vector = np.array(query_embedding)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    results = []\n",
    "    for doc in knowledge_base:\n",
    "        similarity = cosine_similarity(query_vector, doc['embedding'])\n",
    "        if similarity >= threshold:\n",
    "            results.append({\n",
    "                'id': doc['id'],\n",
    "                'source': doc['source'],\n",
    "                'section': doc['section'],\n",
    "                'content': doc['content'],\n",
    "                'similarity': similarity\n",
    "            })\n",
    "    \n",
    "    # Sort by similarity and return top_k\n",
    "    results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "print(\"‚úì Retrieval function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test retrieval\n",
    "test_query = \"How does RAG help reduce hallucinations?\"\n",
    "\n",
    "retrieved_docs = retrieve_documents(test_query, KNOWLEDGE_BASE, top_k=3)\n",
    "\n",
    "print(f\"üîç Query: '{test_query}'\\n\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{i}. [{doc['similarity']:.3f}] {doc['source']} - {doc['section']}\")\n",
    "    print(f\"   {doc['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Context Assembly (Example 2-2)\n",
    "\n",
    "This is the core pattern from Chapter 2: assembling retrieved chunks into coherent context for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Context assembly function (Example 2-2 from Chapter)\n",
    "def assemble_rag_context(query, retrieved_docs, max_context_chars=6000):\n",
    "    \"\"\"\n",
    "    Assemble retrieved document chunks into coherent context for the LLM.\n",
    "    \n",
    "    This implements the context assembly pattern from Example 2-2.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        retrieved_docs: List of retrieved documents with metadata\n",
    "        max_context_chars: Maximum characters for context\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt with assembled context\n",
    "    \"\"\"\n",
    "    if not retrieved_docs:\n",
    "        return f\"\"\"Answer the following question. If you don't have enough information, \n",
    "say so clearly.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Sort by relevance (already sorted, but ensure)\n",
    "    sorted_docs = sorted(retrieved_docs, key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    # Build context with source attribution\n",
    "    context_parts = []\n",
    "    total_chars = 0\n",
    "    sources_used = []\n",
    "    \n",
    "    for doc in sorted_docs:\n",
    "        # Format each chunk with source metadata\n",
    "        chunk_text = f\"\"\"[Source: {doc['source']} | Section: {doc['section']}]\n",
    "{doc['content']}\n",
    "\"\"\"\n",
    "        \n",
    "        # Check if adding this chunk exceeds limit\n",
    "        if total_chars + len(chunk_text) > max_context_chars:\n",
    "            break\n",
    "        \n",
    "        context_parts.append(chunk_text)\n",
    "        total_chars += len(chunk_text)\n",
    "        sources_used.append(f\"{doc['source']} ({doc['section']})\")\n",
    "    \n",
    "    # Assemble the full context\n",
    "    assembled_context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Create the full prompt\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the question using ONLY the context provided below.\n",
    "If the context doesn't contain enough information to answer the question fully, say so.\n",
    "Always cite your sources by mentioning which document the information came from.\n",
    "\n",
    "=== CONTEXT ===\n",
    "{assembled_context}\n",
    "=== END CONTEXT ===\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "- Answer based ONLY on the context above\n",
    "- Cite sources when providing information\n",
    "- If information is missing, acknowledge it\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return prompt, sources_used\n",
    "\n",
    "print(\"‚úì Context assembly function defined (Example 2-2 pattern)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test context assembly\n",
    "prompt, sources = assemble_rag_context(test_query, retrieved_docs)\n",
    "\n",
    "print(\"üìù Assembled Prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(prompt)\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìö Sources used: {len(sources)}\")\n",
    "for source in sources:\n",
    "    print(f\"   - {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generation Component\n",
    "\n",
    "Generate grounded responses using the assembled context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title RAG generation function\n",
    "def generate_rag_response(query, knowledge_base, top_k=3, temperature=0.2):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: retrieve, assemble context, and generate.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        knowledge_base: Document collection with embeddings\n",
    "        top_k: Number of documents to retrieve\n",
    "        temperature: Generation temperature (lower = more focused)\n",
    "    \n",
    "    Returns:\n",
    "        Generated response, sources used, and retrieved documents\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_documents(query, knowledge_base, top_k=top_k)\n",
    "    \n",
    "    # Step 2: Assemble context\n",
    "    prompt, sources = assemble_rag_context(query, retrieved_docs)\n",
    "    \n",
    "    # Step 3: Generate response\n",
    "    generation_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": 1024,\n",
    "    }\n",
    "    \n",
    "    response = generative_model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': response.text,\n",
    "        'sources': sources,\n",
    "        'retrieved_docs': retrieved_docs,\n",
    "        'num_docs_retrieved': len(retrieved_docs)\n",
    "    }\n",
    "\n",
    "print(\"‚úì RAG generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test the complete RAG pipeline\n",
    "QUERY = \"What is RAG and how does it help with LLM applications?\"  # @param {type:\"string\"}\n",
    "\n",
    "result = generate_rag_response(QUERY, KNOWLEDGE_BASE, top_k=4)\n",
    "\n",
    "print(f\"üîç Query: '{QUERY}'\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìù Answer:\")\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìö Sources ({len(result['sources'])}):\")\n",
    "for source in result['sources']:\n",
    "    print(f\"   - {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test with different queries\n",
    "test_queries = [\n",
    "    \"What are the three types of machine learning?\",\n",
    "    \"How do neural networks learn?\",\n",
    "    \"What is the difference between vector search and keyword search?\",\n",
    "    \"What is quantum computing?\"  # Out of scope - should acknowledge\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç Query: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result = generate_rag_response(query, KNOWLEDGE_BASE, top_k=3)\n",
    "    \n",
    "    print(f\"üìù Answer: {result['answer'][:300]}...\")\n",
    "    print(f\"üìö Sources: {result['num_docs_retrieved']} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced: Handling Edge Cases\n",
    "\n",
    "Real-world RAG systems need to handle various edge cases gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Enhanced RAG with edge case handling\n",
    "def enhanced_rag_response(query, knowledge_base, top_k=3, \n",
    "                          similarity_threshold=0.5, \n",
    "                          min_docs_required=1):\n",
    "    \"\"\"\n",
    "    Enhanced RAG pipeline with edge case handling.\n",
    "    \n",
    "    Handles:\n",
    "    - No relevant documents found\n",
    "    - Low confidence responses\n",
    "    - Query classification\n",
    "    \"\"\"\n",
    "    # Retrieve documents with threshold\n",
    "    retrieved_docs = retrieve_documents(\n",
    "        query, \n",
    "        knowledge_base, \n",
    "        top_k=top_k,\n",
    "        threshold=similarity_threshold\n",
    "    )\n",
    "    \n",
    "    # Calculate average similarity\n",
    "    avg_similarity = 0\n",
    "    if retrieved_docs:\n",
    "        avg_similarity = sum(d['similarity'] for d in retrieved_docs) / len(retrieved_docs)\n",
    "    \n",
    "    # Determine confidence level\n",
    "    if len(retrieved_docs) < min_docs_required:\n",
    "        confidence = \"low\"\n",
    "        warning = \"‚ö†Ô∏è Limited relevant information found in knowledge base.\"\n",
    "    elif avg_similarity < 0.6:\n",
    "        confidence = \"medium\"\n",
    "        warning = \"‚ÑπÔ∏è Retrieved documents have moderate relevance.\"\n",
    "    else:\n",
    "        confidence = \"high\"\n",
    "        warning = None\n",
    "    \n",
    "    # Generate response\n",
    "    if len(retrieved_docs) == 0:\n",
    "        return {\n",
    "            'answer': \"I don't have enough information in my knowledge base to answer this question accurately. Please rephrase your question or ask about a different topic.\",\n",
    "            'confidence': 'none',\n",
    "            'sources': [],\n",
    "            'warning': \"‚ùå No relevant documents found.\"\n",
    "        }\n",
    "    \n",
    "    prompt, sources = assemble_rag_context(query, retrieved_docs)\n",
    "    \n",
    "    response = generative_model.generate_content(\n",
    "        prompt,\n",
    "        generation_config={\"temperature\": 0.2, \"max_output_tokens\": 1024}\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': response.text,\n",
    "        'confidence': confidence,\n",
    "        'avg_similarity': avg_similarity,\n",
    "        'sources': sources,\n",
    "        'warning': warning,\n",
    "        'retrieved_docs': retrieved_docs\n",
    "    }\n",
    "\n",
    "print(\"‚úì Enhanced RAG function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Test enhanced RAG with edge cases\nedge_case_queries = [\n    \"Explain deep learning\",  # Should have high confidence\n    \"What is the weather today?\",  # Out of scope\n    \"How does RAG work?\",  # Should have high confidence\n]\n\nfor query in edge_case_queries:\n    print(f\"\\n{'='*60}\")\n    print(f\"üîç Query: {query}\")\n    \n    result = enhanced_rag_response(query, KNOWLEDGE_BASE)\n    \n    if result.get('warning'):\n        print(result['warning'])\n    \n    print(f\"üìä Confidence: {result['confidence']}\")\n    if 'avg_similarity' in result:\n        print(f\"üìà Avg Similarity: {result['avg_similarity']:.3f}\")\n    print(f\"üìù Answer: {result['answer'][:200]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Try It Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add your own documents to the knowledge base\n",
    "custom_docs = [\n",
    "    {\n",
    "        \"id\": \"custom_1\",\n",
    "        \"source\": \"My Custom Document\",\n",
    "        \"section\": \"Introduction\",\n",
    "        \"content\": \"Add your own content here to test the RAG pipeline.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Generate embeddings for custom docs\n",
    "custom_contents = [doc['content'] for doc in custom_docs]\n",
    "custom_embeddings = embedding_model.get_embeddings(custom_contents)\n",
    "\n",
    "for doc, emb in zip(custom_docs, custom_embeddings):\n",
    "    doc['embedding'] = np.array(emb.values)\n",
    "    KNOWLEDGE_BASE.append(doc)\n",
    "\n",
    "print(f\"‚úì Added {len(custom_docs)} custom documents\")\n",
    "print(f\"Total documents in knowledge base: {len(KNOWLEDGE_BASE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different parameters\n",
    "YOUR_QUERY = \"Your question here\"  # @param {type:\"string\"}\n",
    "TOP_K = 3  # @param {type:\"integer\"}\n",
    "SIMILARITY_THRESHOLD = 0.5  # @param {type:\"number\"}\n",
    "\n",
    "result = enhanced_rag_response(\n",
    "    YOUR_QUERY, \n",
    "    KNOWLEDGE_BASE,\n",
    "    top_k=TOP_K,\n",
    "    similarity_threshold=SIMILARITY_THRESHOLD\n",
    ")\n",
    "\n",
    "print(f\"üîç Query: {YOUR_QUERY}\")\n",
    "print(f\"üìä Confidence: {result['confidence']}\")\n",
    "print(f\"üìù Answer:\\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. ‚úÖ **Build a complete RAG pipeline** with retrieval, context assembly, and generation\n",
    "2. ‚úÖ **Implement context assembly** with source attribution (Example 2-2)\n",
    "3. ‚úÖ **Generate grounded responses** using retrieved documents\n",
    "4. ‚úÖ **Handle edge cases** like missing information and low confidence\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Context assembly** is crucial for RAG quality\n",
    "- **Source attribution** improves trustworthiness\n",
    "- **Confidence scoring** helps users understand reliability\n",
    "- **Edge case handling** is essential for production systems\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to the next notebook: **[05_vertex_ai_rag_engine.ipynb](05_vertex_ai_rag_engine.ipynb)** to learn how to use Vertex AI RAG Engine for managed, production-ready RAG."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}