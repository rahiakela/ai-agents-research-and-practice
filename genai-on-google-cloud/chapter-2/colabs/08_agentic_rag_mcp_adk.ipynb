{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQ1nQmSbn9co"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG with MCP and ADK\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ayoisio/genai-on-google-cloud/blob/main/chapter-2/colabs/08_agentic_rag_mcp_adk.ipynb)\n",
    "\n",
    "**Estimated Time**: 30-45 minutes\n",
    "\n",
    "**Prerequisites**: Google Cloud project with billing enabled, Spanner API and Vertex AI API enabled\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **Agentic RAG** - the evolution of RAG where an intelligent agent orchestrates retrieval and generation using the **Model Context Protocol (MCP)** and **Agent Development Kit (ADK)**.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **MCP (Model Context Protocol)**: A standardized protocol for tools to communicate with LLMs\n",
    "- **ADK (Agent Development Kit)**: Google's toolkit for building production-ready AI agents\n",
    "- **Agentic RAG**: Agents that autonomously decide when and how to retrieve information\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "1. **Knowledge Graph Setup**: Spanner Graph with entities and relationships\n",
    "2. **MCP Server**: Tools exposed via Model Context Protocol\n",
    "3. **ADK Agent**: Intelligent agent that uses MCP tools for retrieval\n",
    "4. **Interactive Q&A**: Agent-powered question answering system\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the Agentic RAG pattern and its advantages\n",
    "- Implement MCP servers for tool exposure\n",
    "- Build ADK agents with custom tools\n",
    "- Create autonomous retrieval workflows\n",
    "\n",
    "---\n",
    "\n",
    "**Original Author**: [Tristan Li](https://github.com/codingphun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyXBLYuod9pj"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5wHkD06RzZN"
   },
   "source": [
    "1. In the Google Cloud console, on the project selector page, select or [create a Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects).\n",
    "1. [Make sure that billing is enabled for your Google Cloud project](https://cloud.google.com/billing/docs/how-to/verify-billing-enabled#console).\n",
    "1. [Make sure Cloud Spanner API is enabled](https://console.cloud.google.com/flows/enableapi?apiid=spanner.googleapis.com)\n",
    "\n",
    "### Required roles\n",
    "\n",
    "To get the permissions that you need to complete the tutorial, ask your administrator to grant you the [Owner](https://cloud.google.com/iam/docs/understanding-roles#owner) (`roles/owner`) IAM role on your project. For more information about granting roles, see [Manage access](https://cloud.google.com/iam/docs/granting-changing-revoking-access).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arJM4CK4r6cj"
   },
   "source": [
    "### Install Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg_KM0B08uLb"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-adk==1.11.0 google-genai==1.31.0\n",
    "%pip install --upgrade --quiet mcp==1.13.0\n",
    "%pip install --quiet google-cloud-spanner==3.57.0\n",
    "%pip install --quiet langchain-google-spanner==0.9.0\n",
    "%pip install --quiet langchain-google-vertexai==2.0.28\n",
    "%pip install --quiet json-repair networkx==3.5 langchain-text-splitters==0.3.9 langchain-experimental==0.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoK1gW9dsRxB"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "os3H39sGXugN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "45600d14-5f42-446b-a778-99b36d689fde"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()\n",
    "print(sys.version)\n",
    "# If using local jupyter instance, uncomment and run:\n",
    "# !gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzaWjgsqsuuu"
   },
   "source": [
    "## Initialize and Import"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "GCP_PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "MODEL_NAME = \"gemini-2.5-flash\"  # @param {type:\"string\"}\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-005\"  # @param {type:\"string\"}\n",
    "TASK_TYPE = \"SEMANTIC_SIMILARITY\"  # @param {type:\"string\"}\n",
    "SPANNER_INSTANCE_ID = \"graphrag-instance\"  # @param {type:\"string\"}\n",
    "SPANNER_DATABASE_ID = \"graphrag\"  # @param {type:\"string\"}\n",
    "SPANNER_GRAPH_NAME = \"wikigraph\"  # @param {type:\"string\"}"
   ],
   "metadata": {
    "id": "pthpq0ltRpFg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXDp5uwyd9pk"
   },
   "outputs": [],
   "source": [
    "# Set the project id\n",
    "!gcloud config set project {GCP_PROJECT_ID} --quiet\n",
    "%env GOOGLE_CLOUD_PROJECT={GCP_PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHyZn3Lns9YM"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCQuCvfrXxUM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Axb7c8Y1YmQ8"
   },
   "source": [
    "## Create Spanner Instance and Database\n",
    "\n",
    "To prepare for future queries, we'll now store our newly created knowledge graph in a Google Cloud Spanner database. We'll also store the accompanying embeddings in Spanner's Vector Database to enable efficient semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27gTtXr4m2n2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f6d3b78e-c1f9-464a-db9f-1f8bc0e7149f"
   },
   "outputs": [],
   "source": [
    "!gcloud config set project {GCP_PROJECT_ID}\n",
    "!gcloud services enable spanner.googleapis.com\n",
    "!gcloud spanner instances create {SPANNER_INSTANCE_ID} --config=regional-us-central1 --description=\"Graph RAG Instance\" --nodes=1 --edition=ENTERPRISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsoqBPlDthc7"
   },
   "outputs": [],
   "source": [
    "def create_database(project_id, instance_id, database_id):\n",
    "    \"\"\"Creates a database and tables for sample data.\"\"\"\n",
    "    from google.cloud import spanner\n",
    "    from google.cloud.spanner_admin_database_v1.types import spanner_database_admin\n",
    "\n",
    "    spanner_client = spanner.Client(project_id)\n",
    "    database_admin_api = spanner_client.database_admin_api\n",
    "\n",
    "    request = spanner_database_admin.CreateDatabaseRequest(\n",
    "        parent=database_admin_api.instance_path(spanner_client.project, instance_id),\n",
    "        create_statement=f\"CREATE DATABASE `{database_id}`\",\n",
    "        extra_statements=[\n",
    "            \"\"\"CREATE TABLE KgNode (\n",
    "            DocId        INT64 NOT NULL,\n",
    "            Name STRING(1024),\n",
    "            DOC STRING(1024),\n",
    "            DocEmbedding ARRAY<FLOAT64>\n",
    "            ) PRIMARY KEY (DocId)\"\"\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    operation = database_admin_api.create_database(request=request)\n",
    "\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    OPERATION_TIMEOUT_SECONDS = 60\n",
    "    database = operation.result(OPERATION_TIMEOUT_SECONDS)\n",
    "\n",
    "    print(\n",
    "        \"Created database {} on instance {}\".format(\n",
    "            database.name,\n",
    "            database_admin_api.instance_path(spanner_client.project, instance_id),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mVK78UWt2OJ"
   },
   "outputs": [],
   "source": [
    "from google.cloud import spanner\n",
    "\n",
    "create_database(GCP_PROJECT_ID, SPANNER_INSTANCE_ID, SPANNER_DATABASE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSuWYhb62UJS"
   },
   "source": [
    "## Create a Knowledge Graph With LangChain and Gemini\n",
    "\n",
    "These texts extracted from Wikipedia are about [Larry Page](https://en.wikipedia.org/wiki/Larry_Page), co-founder of Google. These texts will be used to create a knowledge graph about Larry Page as well as embedding vectors for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIokGkZii43z"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Lawrence Edward Page (born March 26, 1973) is an American businessman and computer scientist best known for co-founding Google with Sergey Brin.\n",
    "Lawrence Edward Page was chief executive officer of Google from 1997 until August 2001 when he stepped down in favor of Eric Schmidt,\n",
    "and then again from April 2011 until July 2015 when he became CEO of its newly formed parent organization Alphabet Inc.\n",
    "He held that post until December 4, 2019, when he and Brin stepped down from all executive positions and day-to-day roles within the company.\n",
    "He remains an Alphabet board member, employee, and controlling shareholder. Lawrence Edward Page has an estimated net worth of $156 billion as of June 2024,\n",
    "according to the Bloomberg Billionaires Index, and $145.2 billion according to Forbes, making him the fifth-richest person in the world.\n",
    "He has also invested in flying car startups Kitty Hawk and Opener. Like his Google co-founder, Sergey Brin, Page attended Montessori schools until he entered high school.\n",
    "They both cite the educational method of Maria Montessori as the major influence in how they designed Google's work systems.\n",
    "Maria Montessori believed that the liberty of the child was of utmost importance. In some sense, I feel like music training led to the high-speed legacy of Google for me\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDpwl48k27HU"
   },
   "source": [
    "We will use Gemini and Langchain LLMGraphTransformer to parse the texts and generate a knowledge graph.\n",
    "\n",
    "Leveraging Gemini's capabilities, Langchain will use them to identify and extract key information from the text, such as people, countries, and their nationalities, to construct a comprehensive knowledge graph from the texts based on the nodes and relationships we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIcYPheujsmc"
   },
   "outputs": [],
   "source": [
    "documents = [Document(page_content=text)]\n",
    "\n",
    "llm = VertexAI(model_name=MODEL_NAME, project=GCP_PROJECT_ID, location=REGION)\n",
    "\n",
    "llm_transformer_filtered = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    allowed_nodes=[\"Person\", \"Country\", \"Organization\", \"Asset\"],\n",
    "    allowed_relationships=[\n",
    "        \"NATIONALITY\",\n",
    "        \"LOCATED_IN\",\n",
    "        \"WORKED_AT\",\n",
    "        \"SPOUSE\",\n",
    "        \"NET_WORTH\",\n",
    "        \"INVESTMENT\",\n",
    "        \"INFLUENCED_BY\",\n",
    "    ],\n",
    ")\n",
    "graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(\n",
    "    documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHXfcXadd9pl"
   },
   "source": [
    "## Store the Knowledge Graph in Spanner\n",
    "\n",
    "Now that the Spanner database is created, we will store the Knowledge Graph in the Spanner Graph Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrGjHvf0j7vu"
   },
   "outputs": [],
   "source": [
    "from langchain_google_spanner import SpannerGraphStore\n",
    "\n",
    "graph_store = SpannerGraphStore(\n",
    "    instance_id=SPANNER_INSTANCE_ID,\n",
    "    database_id=SPANNER_DATABASE_ID,\n",
    "    graph_name=SPANNER_GRAPH_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1fV5Y2Ad9pl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a5b1526d-b1c1-4edb-ea9d-08a6a5560c12"
   },
   "outputs": [],
   "source": [
    "# Uncomment the line below, if you want to cleanup from previous iterations.\n",
    "# BeWARE - THIS COULD REMOVE DATA FROM YOUR DATABASE !!!\n",
    "graph_store.cleanup()\n",
    "\n",
    "for graph_document in graph_documents_filtered:\n",
    "    graph_store.add_graph_documents([graph_document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52o_qCYpb1Ut"
   },
   "source": [
    "#### Generate the embeddings\n",
    "\n",
    "We will now generate embeddings for vector search part of the GraphRAG. For more information on how this works, check out [GraphRAG infrastructure using Vertex AI and Spanner Graph](https://cloud.google.com/architecture/gen-ai-graphrag-spanner) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7XGJrb2d9pm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b183b99c-79f7-4ff6-f6c2-906e9e7cfe53"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from google import genai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def get_embedding(embtext, client):\n",
    "    try:\n",
    "        response = client.models.embed_content(\n",
    "          model=EMBEDDING_MODEL_NAME, contents=embtext, config=genai.types.EmbedContentConfig(task_type=TASK_TYPE)\n",
    "        )\n",
    "        return response.embeddings[0].values\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "client = genai.Client(vertexai=True, project=GCP_PROJECT_ID, location=REGION)\n",
    "documents = [Document(page_content=text)]\n",
    "spanner_embedding_values = []\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", r\"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "splitted_text = splitter.split_documents(documents)\n",
    "for chunk in splitted_text:\n",
    "    chunk_content = chunk.page_content\n",
    "    embedding = get_embedding(chunk_content, client)\n",
    "    user_prompt_content = (\n",
    "        \"Find person's names but ignore any pronoun in the following sentence \\n\"\n",
    "        + chunk_content\n",
    "    )\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=user_prompt_content,\n",
    "        config=genai.types.GenerateContentConfig(\n",
    "            temperature=0.1,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema={\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\"nodes\": {\"type\": \"STRING\"}},\n",
    "            },\n",
    "            thinking_config=genai.types.ThinkingConfig(\n",
    "                thinking_budget=-1\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    response_content = json.loads(response.candidates[0].content.parts[0].text)[\"nodes\"]\n",
    "    print(embedding[:10], chunk_content[:10], response_content)\n",
    "    spanner_embedding_values.append([embedding, chunk_content, response_content])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OVOZZ1Ld9pm"
   },
   "source": [
    "#### Save the embeddings into Spanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K4tCvssd9pm"
   },
   "outputs": [],
   "source": [
    "from google.cloud import spanner\n",
    "\n",
    "spanner_client = spanner.Client(GCP_PROJECT_ID)\n",
    "instance = spanner_client.instance(SPANNER_INSTANCE_ID)\n",
    "database = instance.database(SPANNER_DATABASE_ID)\n",
    "\n",
    "def insert_values(transaction):\n",
    "    value1 = 0\n",
    "    for sub_list in spanner_embedding_values:\n",
    "        table_name = \"KgNode\"\n",
    "        col_name1 = \"docID\"\n",
    "        col_name2 = \"Name\"\n",
    "        col_name3 = \"Doc\"\n",
    "        col_name4 = \"DocEmbedding\"\n",
    "        value1 += 1\n",
    "        value2 = sub_list[2]\n",
    "        value3 = sub_list[1]\n",
    "        value4 = sub_list[0]\n",
    "        # print(col_name1, col_name2, col_name3, col_name4, value1, value2, value3, value4[:10])\n",
    "        row_ct1 = transaction.execute_update(\n",
    "            f\"INSERT INTO {table_name} ({col_name1}, {col_name2}, {col_name3}, {col_name4}) VALUES (@value1, @value2, @value3, @value4)\",\n",
    "            params={\n",
    "                \"value1\": value1,\n",
    "                \"value2\": value2,\n",
    "                \"value3\": value3,\n",
    "                \"value4\": value4,\n",
    "            },\n",
    "            param_types={\n",
    "                \"value1\": spanner.param_types.INT64,\n",
    "                \"value2\": spanner.param_types.STRING,\n",
    "                \"value3\": spanner.param_types.STRING,\n",
    "                \"value4\": spanner.param_types.Array(spanner.param_types.FLOAT64),\n",
    "            },\n",
    "        )  # Adjust types if needed\n",
    "\n",
    "        print(f\"{row_ct1} record(s) inserted.\")\n",
    "\n",
    "# print(insert_values)  # This just prints the function object, remove this line\n",
    "database.run_in_transaction(insert_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build a MCP server and ADK Q&A agent\n",
    "\n",
    "Now that the knowledge graph and embeddings are stored in Spanner. We will go ahead and build a MCP server to retrieve the data and return the result back to an ADK Q&A agent."
   ],
   "metadata": {
    "id": "oYJOwpYN3-y8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate the MCP server code\n",
    "\n",
    "***Be sure*** to add your project ID in the server code below. For production deployment, these parameters can be set in either enviornment variable or through configuration file. See [Enviornment-Specific Configurations](https://google.github.io/adk-docs/tools/mcp-tools/#cloud-run_1) for more detail."
   ],
   "metadata": {
    "id": "osuzDH8p8wNE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile my_mcp_server.py\n",
    "\n",
    "# my_mcp_server.py - Can be deployed to Cloud Run service using Streamable HTTP\n",
    "import contextlib\n",
    "import logging\n",
    "from collections.abc import AsyncIterator\n",
    "from typing import Any\n",
    "\n",
    "import anyio\n",
    "import click\n",
    "import mcp.types as types\n",
    "from mcp.server.lowlevel import Server\n",
    "from mcp.server.streamable_http_manager import StreamableHTTPSessionManager\n",
    "from starlette.applications import Starlette\n",
    "from starlette.routing import Mount\n",
    "from starlette.types import Receive, Scope, Send\n",
    "\n",
    "import json\n",
    "import os\n",
    "from google.cloud import spanner\n",
    "from google import genai\n",
    "from langchain_google_spanner import SpannerGraphQAChain, SpannerGraphStore\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_mcp_server():\n",
    "    \"\"\"Create and configure the MCP server.\"\"\"\n",
    "    app = Server(\"adk-mcp-streamable-server\")\n",
    "\n",
    "    async def ask_graph(query: str) -> dict:\n",
    "\n",
    "      GCP_PROJECT_ID = \"\"\n",
    "      REGION = \"us-central1\"\n",
    "      MODEL_NAME = \"gemini-2.5-flash\"\n",
    "      EMBEDDING_MODEL_NAME = \"text-embedding-005\"\n",
    "      TASK_TYPE = \"SEMANTIC_SIMILARITY\"\n",
    "      SPANNER_INSTANCE_ID = \"graphrag-instance\"\n",
    "      SPANNER_DATABASE_ID = \"graphrag\"\n",
    "      SPANNER_GRAPH_NAME = \"wikigraph\"\n",
    "\n",
    "      graph_store = SpannerGraphStore(\n",
    "          instance_id=SPANNER_INSTANCE_ID,\n",
    "          database_id=SPANNER_DATABASE_ID,\n",
    "          graph_name=SPANNER_GRAPH_NAME,\n",
    "      )\n",
    "\n",
    "      # Initialize llm object\n",
    "      llm = ChatVertexAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "      # Initialize GraphQAChain\n",
    "      chain = SpannerGraphQAChain.from_llm(\n",
    "          llm,\n",
    "          graph=graph_store,\n",
    "          allow_dangerous_requests=True,\n",
    "          verbose=False,\n",
    "          return_intermediate_steps=True,\n",
    "      )\n",
    "      client = genai.Client(vertexai=True, project=GCP_PROJECT_ID, location=REGION)\n",
    "      response = client.models.embed_content(\n",
    "          model=EMBEDDING_MODEL_NAME, contents=query, config=genai.types.EmbedContentConfig(task_type=TASK_TYPE)\n",
    "      )\n",
    "      q_emb = response.embeddings[0].values\n",
    "      spanner_client = spanner.Client(GCP_PROJECT_ID)\n",
    "      instance = spanner_client.instance(SPANNER_INSTANCE_ID)\n",
    "      database = instance.database(SPANNER_DATABASE_ID)\n",
    "      kgnodename = \"\"\n",
    "      with database.snapshot() as snapshot:\n",
    "          results = snapshot.execute_sql(\n",
    "              \"\"\"SELECT DocId, NAME, Doc FROM KgNode ORDER BY COSINE_DISTANCE(DocEmbedding, @q_emb) limit 1\"\"\",\n",
    "              params={\"q_emb\": q_emb},\n",
    "              param_types={\n",
    "                  \"q_emb\": spanner.param_types.Array(spanner.param_types.FLOAT64)\n",
    "              },  # Adjust FLOAT64 if needed\n",
    "          )\n",
    "          for row in results:\n",
    "              kgnodename = str(row[1])\n",
    "\n",
    "      user_prompt_content = (\n",
    "          \"Find and replace entities such as person's name, place, nationality in the following sentence \\n\"\n",
    "          + query\n",
    "          + \"with entities defined below \\n\"\n",
    "          + kgnodename\n",
    "          + \"\\n only replace matching person's name \\n output the best replacement in a string\"\n",
    "      )\n",
    "      response = client.models.generate_content(\n",
    "          model=MODEL_NAME,\n",
    "          contents=user_prompt_content,\n",
    "          config=genai.types.GenerateContentConfig(\n",
    "              temperature=0.1,\n",
    "              response_mime_type=\"application/json\",\n",
    "              response_schema={\n",
    "                  \"type\": \"OBJECT\",\n",
    "                  \"properties\": {\"sentence\": {\"type\": \"STRING\"}},\n",
    "              },\n",
    "              thinking_config=genai.types.ThinkingConfig(\n",
    "                thinking_budget=-1\n",
    "              )\n",
    "          ),\n",
    "      )\n",
    "      response_content = json.loads(response.candidates[0].content.parts[0].text)[\"sentence\"]\n",
    "      response = chain.invoke(\"query=\" + response_content)\n",
    "      response[\"result\"]\n",
    "      return response[\"result\"]\n",
    "\n",
    "    @app.call_tool()\n",
    "    async def call_tool(name: str, arguments: dict[str, Any]) -> list[types.ContentBlock]:\n",
    "        \"\"\"Handle tool calls from MCP clients.\"\"\"\n",
    "\n",
    "        # Example tool implementation - replace with your actual ADK tools\n",
    "        if name == \"ask_graph\":\n",
    "            query = arguments.get(\"query\", \"No input provided\")\n",
    "            answer = await ask_graph(query)\n",
    "            logger.info(answer)\n",
    "            return [\n",
    "                types.TextContent(\n",
    "                    type=\"text\",\n",
    "                    text=answer\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tool: {name}\")\n",
    "\n",
    "    @app.list_tools()\n",
    "    async def list_tools() -> list[types.Tool]:\n",
    "        logger.info(\"List available tools.\")\n",
    "\n",
    "\n",
    "        return [\n",
    "            types.Tool(\n",
    "                name=\"ask_graph\",\n",
    "                description=\"Tool to call Knowledge Graph stored in the Spanner database\",\n",
    "                inputSchema={\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The query string to process.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return app\n",
    "\n",
    "def main(port: int = 8081, json_response: bool = False):\n",
    "    \"\"\"Main server function.\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    app = create_mcp_server()\n",
    "\n",
    "    # Create session manager with stateless mode for scalability\n",
    "    session_manager = StreamableHTTPSessionManager(\n",
    "        app=app,\n",
    "        event_store=None,\n",
    "        json_response=json_response,\n",
    "        stateless=True,  # Important for Cloud Run scalability\n",
    "    )\n",
    "\n",
    "    async def handle_streamable_http(scope: Scope, receive: Receive, send: Send) -> None:\n",
    "        await session_manager.handle_request(scope, receive, send)\n",
    "\n",
    "    @contextlib.asynccontextmanager\n",
    "    async def lifespan(app: Starlette) -> AsyncIterator[None]:\n",
    "        \"\"\"Manage session manager lifecycle.\"\"\"\n",
    "        async with session_manager.run():\n",
    "            logger.info(\"MCP Streamable HTTP server started!\")\n",
    "            try:\n",
    "                yield\n",
    "            finally:\n",
    "                logger.info(\"MCP server shutting down...\")\n",
    "\n",
    "    # Create ASGI application\n",
    "    starlette_app = Starlette(\n",
    "        debug=False,  # Set to False for production\n",
    "        routes=[\n",
    "            Mount(\"/mcp\", app=handle_streamable_http),\n",
    "        ],\n",
    "        lifespan=lifespan,\n",
    "    )\n",
    "\n",
    "    import uvicorn\n",
    "    uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GfIxW2F7nrY",
    "outputId": "90898ac8-9ab3-4cce-db62-900722bad350"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run the MCP server locally\n",
    "\n",
    "Start a terminal and navigate to the the folder where this notebook is stored. Run the following commands to setup the enviornment for the MCP server code to run properly:"
   ],
   "metadata": {
    "id": "o7cnGHrSo_iG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "`gcloud auth application-default login`\n",
    "\n",
    "`gcloud auth application-default set-quota-project your_project_id`\n",
    "\n",
    "`gcloud config set project your_project_id`\n",
    "\n",
    "`uv run my_mcp_server.py`"
   ],
   "metadata": {
    "id": "ezgTjU7y92fS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Setup ADK client code\n",
    "\n",
    "With the MCP server running, we can now setup an Agent with ADK that can get information stored in Spanner Graph through MCP tool calling."
   ],
   "metadata": {
    "id": "MZg01IaY9Bfk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"1\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = GCP_PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = REGION"
   ],
   "metadata": {
    "id": "TrxKEcKxo1dD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.adk.agents.llm_agent import LlmAgent\n",
    "from google.adk.tools.mcp_tool.mcp_toolset import (\n",
    "    MCPToolset,\n",
    "    StreamableHTTPConnectionParams\n",
    ")\n",
    "\n",
    "toolset = MCPToolset(\n",
    "    connection_params=StreamableHTTPConnectionParams(\n",
    "        url=\"http://localhost:8081/mcp\",\n",
    "        headers={'Accept': 'text/event-stream'}\n",
    "        #add \"Authorization\": \"Bearer your-auth-token\" for authroization\n",
    "    )\n",
    ")\n",
    "root_agent = LlmAgent(\n",
    "    model=MODEL_NAME,\n",
    "    name=\"graph_rag_agent\",\n",
    "    description=(\n",
    "        \"Agent to answer questions from a MCP knowledge graph server\"\n",
    "    ),\n",
    "    instruction=(\n",
    "        \"\"\"You are a helpful information retrieval agent that can answer user's query from a MCP\n",
    "        knowledge graph server provided by the following tool call.\n",
    "          - If the query can be answered from the graph, then call the ask_graph tool.\n",
    "          - Always be courteous and dont assume anything.\n",
    "          - If you dont know the answer, say I dont know the answer.\"\"\"\n",
    "    ),\n",
    "    tools=[toolset]\n",
    ")"
   ],
   "metadata": {
    "id": "C5pIBzcl0Kbc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97f26922-dce0-49b5-9ab7-d252c51391ee"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.adk.tools import google_search\n",
    "from google.genai import types\n",
    "\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "\n",
    "APP_NAME=\"google_search_agent\"\n",
    "USER_ID=\"user1234\"\n",
    "SESSION_ID=\"1234\"\n",
    "\n",
    "async def run_agent():\n",
    "  try:\n",
    "    session_service = InMemorySessionService()\n",
    "    session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
    "    runner = Runner(agent=root_agent, app_name=APP_NAME, session_service=session_service)\n",
    "\n",
    "    query=\"What businesses does Larry Page invest in?\"\n",
    "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "    async for event in runner.run_async(\n",
    "        user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
    "    ):\n",
    "        if event.is_final_response():\n",
    "            final_response = event.content.parts[0].text\n",
    "            print(\"Agent Response: \", final_response)\n",
    "  except Exception as e:\n",
    "      print(f\"A general error occurred: {e}\")\n",
    "\n",
    "await run_agent()"
   ],
   "metadata": {
    "id": "cF_1FdsnDo7d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3eb071f4-58b8-4385-f322-dc59c4288475"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaY6vhFE9egF"
   },
   "source": [
    "## Clean Up\n",
    "\n",
    "*   Delete the Spanner instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qb4FcKf9dS-d"
   },
   "outputs": [],
   "source": [
    "!gcloud spanner instances delete {SPANNER_INSTANCE_ID} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKFKrjkud9pn"
   },
   "source": [
    "## What's next\n",
    "\n",
    "* GraphRAG infrastructure for generative AI using [Vertex AI and Spanner Graph](https://cloud.google.com/architecture/gen-ai-graphrag-spanner).\n",
    "* Dive deeper into [LangChain with Spanner](https://github.com/googleapis/langchain-google-spanner-python/tree/main).\n",
    "* Learn more about [Spanner](https://cloud.google.com/spanner/docs/getting-started/python).\n",
    "* Explore other [Spanner Graph Notebooks](https://github.com/cloudspannerecosystem/spanner-graph-notebook/blob/main/README.md).\n",
    "* Learn more about [Agent Development Kit](https://google.github.io/adk-docs/)."
   ]
  }
 ]
}