{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managed RAG with Vertex AI RAG Engine\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ayoisio/genai-on-google-cloud/blob/main/chapter-2/colabs/05_vertex_ai_rag_engine.ipynb)\n",
    "\n",
    "**Estimated Time**: 15-20 minutes\n",
    "\n",
    "**Prerequisites**: Google Cloud project with billing enabled, Vertex AI API enabled\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Vertex AI RAG Engine provides a fully managed solution for Retrieval-Augmented Generation. This notebook demonstrates:\n",
    "\n",
    "1. **Create a RAG corpus** for document storage and indexing\n",
    "2. **Upload and index documents** from various sources\n",
    "3. **Query with RAG retrieval** using GenerativeModel\n",
    "4. **Compare DIY vs managed RAG** approaches\n",
    "\n",
    "RAG Engine handles document chunking, embedding generation, vector indexing, and retrieval automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Install Dependencies\n!pip install --upgrade google-cloud-aiplatform google-cloud-storage -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Authenticate with Google Cloud\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "print(\"âœ“ Authentication successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configure Your Project\n",
    "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Validate project ID\n",
    "if PROJECT_ID == \"your-project-id\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID above\")\n",
    "\n",
    "print(f\"âœ“ Project: {PROJECT_ID}\")\n",
    "print(f\"âœ“ Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Initialize Vertex AI\nimport vertexai\nfrom vertexai.preview import rag  # RAG is in preview module\nfrom vertexai.generative_models import GenerativeModel, Tool\n\nvertexai.init(project=PROJECT_ID, location=LOCATION)\n\nprint(f\"âœ“ Vertex AI initialized\")\nprint(f\"âœ“ RAG Engine ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Understanding RAG Engine Architecture\n\nVertex AI RAG Engine provides:\n\n- **RAG Corpus**: A managed container for your documents and their embeddings\n- **RAG Files**: Individual documents uploaded to a corpus\n- **Automatic Processing**: Chunking, embedding, and indexing handled automatically\n- **Retrieval Tool**: Integration with GenerativeModel for grounded responses\n\n```mermaid\nflowchart LR\n    A[Documents] --> B[RAG Corpus]\n    B --> C[Embeddings]\n    C --> D[Vector Index]\n    D --> E[Retrieval]\n    E --> F[Gemini]\n    F --> G[Response]\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a RAG Corpus\n",
    "\n",
    "A RAG Corpus is a container that stores your documents along with their embeddings for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create a new RAG corpus\n",
    "CORPUS_DISPLAY_NAME = \"chapter2_demo_corpus\"  # @param {type:\"string\"}\n",
    "CORPUS_DESCRIPTION = \"Demo corpus for Chapter 2 RAG examples\"  # @param {type:\"string\"}\n",
    "\n",
    "# Configure embedding model for the corpus\n",
    "embedding_model_config = rag.EmbeddingModelConfig(\n",
    "    publisher_model=\"publishers/google/models/text-embedding-005\"\n",
    ")\n",
    "\n",
    "# Create the corpus\n",
    "rag_corpus = rag.create_corpus(\n",
    "    display_name=CORPUS_DISPLAY_NAME,\n",
    "    description=CORPUS_DESCRIPTION,\n",
    "    embedding_model_config=embedding_model_config,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Created RAG Corpus\")\n",
    "print(f\"  Name: {rag_corpus.name}\")\n",
    "print(f\"  Display Name: {rag_corpus.display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title List existing corpora (optional)\n",
    "corpora = rag.list_corpora()\n",
    "\n",
    "print(\"ğŸ“š Existing RAG Corpora:\")\n",
    "for corpus in corpora:\n",
    "    print(f\"  - {corpus.display_name}: {corpus.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Documents to the Corpus\n",
    "\n",
    "RAG Engine supports multiple document sources:\n",
    "- **Cloud Storage**: GCS URIs for files\n",
    "- **Google Drive**: Drive folder/file IDs\n",
    "- **Inline text**: Direct text upload\n",
    "\n",
    "Documents are automatically chunked, embedded, and indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create sample documents for upload\n",
    "# We'll create sample documents inline for this demo\n",
    "\n",
    "SAMPLE_DOCUMENTS = [\n",
    "    {\n",
    "        \"display_name\": \"ml_fundamentals.txt\",\n",
    "        \"content\": \"\"\"Machine Learning Fundamentals\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. The core idea is to develop algorithms that can access data and use it to learn for themselves.\n",
    "\n",
    "There are three main types of machine learning:\n",
    "\n",
    "1. Supervised Learning: Uses labeled data to train models. The algorithm learns from example input-output pairs to predict outputs for new inputs. Common applications include classification and regression.\n",
    "\n",
    "2. Unsupervised Learning: Finds patterns in data without labels. The algorithm discovers hidden structures in unlabeled data. Common applications include clustering and dimensionality reduction.\n",
    "\n",
    "3. Reinforcement Learning: Learns through rewards and penalties. An agent learns to make decisions by performing actions and receiving feedback. Common applications include game playing and robotics.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"display_name\": \"deep_learning_intro.txt\",\n",
    "        \"content\": \"\"\"Introduction to Deep Learning\n",
    "\n",
    "Deep learning uses neural networks with multiple layers (hence 'deep') to progressively extract higher-level features from raw input. For example, in image recognition, lower layers might identify edges, while higher layers identify concepts like faces or objects.\n",
    "\n",
    "Training a neural network involves:\n",
    "- Forward Propagation: Computing predictions by passing input through the network\n",
    "- Loss Calculation: Measuring the error between predictions and actual values\n",
    "- Backpropagation: Adjusting weights to minimize the loss\n",
    "\n",
    "This process repeats over many iterations (epochs) until the model converges to an optimal solution. Deep learning has revolutionized fields like computer vision, natural language processing, and speech recognition.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"display_name\": \"rag_overview.txt\",\n",
    "        \"content\": \"\"\"Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "RAG combines the power of large language models with external knowledge retrieval. Instead of relying solely on the model's trained knowledge, RAG retrieves relevant documents and uses them to generate more accurate, up-to-date responses.\n",
    "\n",
    "Key Benefits of RAG:\n",
    "- Reduced Hallucinations: Responses are grounded in real documents\n",
    "- Source Citations: Can cite specific sources for information\n",
    "- Easy Updates: Knowledge can be updated without retraining the model\n",
    "- Domain Accuracy: Provides accurate information for specific domains\n",
    "\n",
    "RAG is particularly valuable for enterprise applications where accuracy, currency, and traceability of information are critical. It bridges the gap between general LLM knowledge and specific organizational knowledge.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"display_name\": \"vertex_ai_features.txt\",\n",
    "        \"content\": \"\"\"Vertex AI Platform Features\n",
    "\n",
    "Vertex AI is Google Cloud's unified platform for machine learning and AI. It provides a comprehensive suite of tools for building, deploying, and managing AI applications.\n",
    "\n",
    "Key Features:\n",
    "- Generative AI: Access to Gemini models for text, code, and multimodal generation\n",
    "- Model Garden: Pre-trained models for various tasks\n",
    "- RAG Engine: Managed retrieval-augmented generation\n",
    "- Vector Search: High-performance similarity search\n",
    "- Feature Store: Centralized feature management\n",
    "- Pipelines: MLOps workflow orchestration\n",
    "- Experiments: Track and compare model versions\n",
    "\n",
    "Vertex AI integrates seamlessly with BigQuery, Cloud Storage, and other Google Cloud services, enabling end-to-end AI workflows at enterprise scale.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Prepared {len(SAMPLE_DOCUMENTS)} sample documents for upload\")\n",
    "for doc in SAMPLE_DOCUMENTS:\n",
    "    print(f\"  - {doc['display_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save sample documents to Cloud Storage (for upload)\n",
    "from google.cloud import storage\n",
    "import uuid\n",
    "\n",
    "# Create a unique bucket name\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-rag-demo-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# Initialize storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Create bucket\n",
    "try:\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME, location=LOCATION)\n",
    "    print(f\"âœ“ Created bucket: {BUCKET_NAME}\")\n",
    "except Exception as e:\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    print(f\"Using existing bucket: {BUCKET_NAME}\")\n",
    "\n",
    "# Upload documents\n",
    "gcs_uris = []\n",
    "for doc in SAMPLE_DOCUMENTS:\n",
    "    blob = bucket.blob(f\"documents/{doc['display_name']}\")\n",
    "    blob.upload_from_string(doc['content'], content_type='text/plain')\n",
    "    gcs_uri = f\"gs://{BUCKET_NAME}/documents/{doc['display_name']}\"\n",
    "    gcs_uris.append(gcs_uri)\n",
    "    print(f\"  Uploaded: {gcs_uri}\")\n",
    "\n",
    "print(f\"\\nâœ“ Uploaded {len(gcs_uris)} documents to Cloud Storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import documents into RAG corpus\n",
    "# Import documents from Cloud Storage into the corpus\n",
    "\n",
    "# Configure chunking (optional - using defaults)\n",
    "# RAG Engine will automatically chunk documents\n",
    "\n",
    "response = rag.import_files(\n",
    "    corpus_name=rag_corpus.name,\n",
    "    paths=gcs_uris,\n",
    "    chunk_size=512,  # Characters per chunk\n",
    "    chunk_overlap=100,  # Overlap between chunks\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Import completed\")\n",
    "print(f\"  Imported file count: {response.imported_rag_files_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title List files in the corpus\n",
    "rag_files = rag.list_files(corpus_name=rag_corpus.name)\n",
    "\n",
    "print(\"ğŸ“„ Files in corpus:\")\n",
    "for file in rag_files:\n",
    "    print(f\"  - {file.display_name}\")\n",
    "    print(f\"    Name: {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query the RAG Corpus\n",
    "\n",
    "Now let's query our corpus using two methods:\n",
    "1. **Direct Retrieval**: Get relevant chunks without generation\n",
    "2. **RAG-Grounded Generation**: Use retrieved context for LLM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Direct retrieval from corpus\n",
    "QUERY = \"What are the benefits of RAG?\"  # @param {type:\"string\"}\n",
    "\n",
    "# Retrieve relevant contexts\n",
    "response = rag.retrieval_query(\n",
    "    rag_resources=[\n",
    "        rag.RagResource(\n",
    "            rag_corpus=rag_corpus.name,\n",
    "        )\n",
    "    ],\n",
    "    text=QUERY,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "print(f\"ğŸ” Query: '{QUERY}'\\n\")\n",
    "print(\"Retrieved contexts:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, context in enumerate(response.contexts.contexts, 1):\n",
    "    print(f\"\\n[{i}] Score: {context.distance:.4f}\")\n",
    "    print(f\"Source: {context.source_uri}\")\n",
    "    print(f\"Text: {context.text[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title RAG-grounded generation with Gemini\n",
    "# Create a RAG retrieval tool\n",
    "rag_retrieval_tool = Tool.from_retrieval(\n",
    "    retrieval=rag.Retrieval(\n",
    "        source=rag.VertexRagStore(\n",
    "            rag_resources=[\n",
    "                rag.RagResource(\n",
    "                    rag_corpus=rag_corpus.name,\n",
    "                )\n",
    "            ],\n",
    "            similarity_top_k=3,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create model with RAG tool\n",
    "rag_model = GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash\",\n",
    "    tools=[rag_retrieval_tool],\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG-enabled model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate RAG-grounded response\n",
    "QUESTION = \"What is RAG and what are its key benefits for enterprise applications?\"  # @param {type:\"string\"}\n",
    "\n",
    "response = rag_model.generate_content(QUESTION)\n",
    "\n",
    "print(f\"ğŸ” Question: {QUESTION}\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“ RAG-Grounded Response:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test with multiple questions\n",
    "test_questions = [\n",
    "    \"What are the three types of machine learning?\",\n",
    "    \"How does deep learning training work?\",\n",
    "    \"What features does Vertex AI provide?\",\n",
    "    \"How does backpropagation work in neural networks?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” Question: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    response = rag_model.generate_content(question)\n",
    "    print(f\"ğŸ“ Answer: {response.text[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DIY RAG vs Managed RAG Comparison\n",
    "\n",
    "Let's compare the approaches we've learned in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Comparison: DIY RAG vs Vertex AI RAG Engine\n",
    "comparison = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DIY RAG vs Vertex AI RAG Engine                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Aspect               â”‚ DIY RAG                â”‚ Vertex AI RAG Engine        â”‚\n",
    "â”‚                      â”‚ (Notebook 04)          â”‚ (This Notebook)             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Setup Complexity     â”‚ High - manage all      â”‚ Low - managed service       â”‚\n",
    "â”‚                      â”‚ components yourself    â”‚                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Chunking             â”‚ Custom implementation  â”‚ Automatic with config       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Embeddings           â”‚ Generate & store       â”‚ Managed automatically       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Vector Index         â”‚ Build & maintain       â”‚ Managed automatically       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Retrieval            â”‚ Custom similarity      â”‚ Built-in retrieval_query    â”‚\n",
    "â”‚                      â”‚ search logic           â”‚                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ LLM Integration      â”‚ Manual prompt          â”‚ Tool.from_retrieval()       â”‚\n",
    "â”‚                      â”‚ engineering            â”‚                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Scaling              â”‚ Manual infrastructure  â”‚ Auto-scaling                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Customization        â”‚ Full control           â”‚ Limited to config options   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Best For             â”‚ Custom requirements,   â”‚ Quick deployment,           â”‚\n",
    "â”‚                      â”‚ specialized logic      â”‚ production workloads        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title When to use each approach\n",
    "guidance = \"\"\"\n",
    "ğŸ“‹ Choosing Your RAG Approach\n",
    "\n",
    "âœ… Use DIY RAG (Notebook 04) when:\n",
    "   â€¢ You need custom chunking strategies\n",
    "   â€¢ You require specialized embedding models\n",
    "   â€¢ You need fine-grained control over retrieval logic\n",
    "   â€¢ You're integrating with existing vector databases\n",
    "   â€¢ You need custom re-ranking or filtering\n",
    "\n",
    "âœ… Use Vertex AI RAG Engine (This notebook) when:\n",
    "   â€¢ You want rapid deployment to production\n",
    "   â€¢ You prefer managed infrastructure\n",
    "   â€¢ Standard chunking and embedding work for your use case\n",
    "   â€¢ You need built-in scaling and reliability\n",
    "   â€¢ You want tight integration with Gemini models\n",
    "\n",
    "ğŸ’¡ Hybrid Approach:\n",
    "   Many production systems use both - custom preprocessing\n",
    "   with managed retrieval and generation.\n",
    "\"\"\"\n",
    "\n",
    "print(guidance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Configuration (Reference)\n",
    "\n",
    "RAG Engine provides various configuration options for fine-tuning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Advanced RAG configuration patterns\n",
    "ADVANCED_PATTERNS = '''\n",
    "# 1. Custom Embedding Model\n",
    "embedding_config = rag.EmbeddingModelConfig(\n",
    "    publisher_model=\"publishers/google/models/text-embedding-005\",\n",
    "    # Or use a custom endpoint:\n",
    "    # endpoint=\"projects/{project}/locations/{location}/endpoints/{endpoint_id}\"\n",
    ")\n",
    "\n",
    "# 2. Custom Chunking Configuration\n",
    "# When importing files:\n",
    "response = rag.import_files(\n",
    "    corpus_name=corpus.name,\n",
    "    paths=[\"gs://bucket/document.pdf\"],\n",
    "    chunk_size=1024,       # Characters per chunk\n",
    "    chunk_overlap=200,     # Overlap between chunks\n",
    ")\n",
    "\n",
    "# 3. Retrieval with Filtering\n",
    "response = rag.retrieval_query(\n",
    "    rag_resources=[\n",
    "        rag.RagResource(\n",
    "            rag_corpus=corpus.name,\n",
    "            # Filter to specific files:\n",
    "            rag_file_ids=[\"file-id-1\", \"file-id-2\"],\n",
    "        )\n",
    "    ],\n",
    "    text=\"query\",\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "# 4. Multiple Corpora\n",
    "# Query across multiple knowledge bases:\n",
    "rag_retrieval_tool = Tool.from_retrieval(\n",
    "    retrieval=rag.Retrieval(\n",
    "        source=rag.VertexRagStore(\n",
    "            rag_resources=[\n",
    "                rag.RagResource(rag_corpus=\"corpus-1\"),\n",
    "                rag.RagResource(rag_corpus=\"corpus-2\"),\n",
    "            ],\n",
    "            similarity_top_k=5,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Data Sources\n",
    "# Import from Google Drive:\n",
    "response = rag.import_files(\n",
    "    corpus_name=corpus.name,\n",
    "    paths=[\"https://drive.google.com/file/d/{file_id}\"],\n",
    ")\n",
    "\n",
    "# Import from Slack:\n",
    "# rag.import_files_async(..., source=rag.SlackChannelsSource(...))\n",
    "\n",
    "# Import from Jira:\n",
    "# rag.import_files_async(..., source=rag.JiraSource(...))\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“‹ Advanced RAG Configuration Patterns:\")\n",
    "print(ADVANCED_PATTERNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cleanup resources\n",
    "CLEANUP = False  # @param {type:\"boolean\"}\n",
    "\n",
    "if CLEANUP:\n",
    "    # Delete RAG corpus (this also deletes all files in it)\n",
    "    try:\n",
    "        rag.delete_corpus(name=rag_corpus.name)\n",
    "        print(f\"âœ“ Deleted RAG corpus: {rag_corpus.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete corpus: {e}\")\n",
    "    \n",
    "    # Delete Cloud Storage bucket\n",
    "    try:\n",
    "        bucket = storage_client.bucket(BUCKET_NAME)\n",
    "        bucket.delete(force=True)\n",
    "        print(f\"âœ“ Deleted bucket: {BUCKET_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete bucket: {e}\")\n",
    "else:\n",
    "    print(\"Skipping cleanup. Set CLEANUP=True to delete resources.\")\n",
    "    print(f\"\\nâš ï¸ Remember to delete these resources when done:\")\n",
    "    print(f\"  - RAG Corpus: {rag_corpus.name}\")\n",
    "    print(f\"  - GCS Bucket: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. âœ… **Create a RAG corpus** with custom embedding configuration\n",
    "2. âœ… **Upload and index documents** from Cloud Storage\n",
    "3. âœ… **Perform direct retrieval** to find relevant contexts\n",
    "4. âœ… **Generate RAG-grounded responses** using Gemini with retrieval tools\n",
    "5. âœ… **Compare DIY vs managed RAG** approaches\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **RAG Engine** provides a fully managed RAG solution\n",
    "- **Automatic processing** handles chunking, embeddings, and indexing\n",
    "- **Tool.from_retrieval()** seamlessly integrates RAG with Gemini\n",
    "- **Multiple data sources** are supported (GCS, Drive, Slack, Jira)\n",
    "- **Choose your approach** based on customization vs simplicity needs\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter 2 Complete!\n",
    "\n",
    "Congratulations! You've completed all the hands-on exercises for Chapter 2: Data Readiness and Accessibility.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "| Notebook | Key Skills |\n",
    "|----------|------------|\n",
    "| 01 - Data Exploration | BigQuery profiling, data quality assessment |\n",
    "| 02 - Document Processing | Gemini multimodal, Document AI patterns |\n",
    "| 03 - Embeddings & Vector Search | Text embeddings, semantic similarity |\n",
    "| 04 - RAG Context Assembly | Complete RAG pipeline, context assembly |\n",
    "| 05 - RAG Engine | Managed RAG, production deployment |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **Chapter 3** to learn about building AI Agents with ADK (Agent Development Kit)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}