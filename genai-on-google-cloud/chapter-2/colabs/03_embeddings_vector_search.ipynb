{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embeddings and Vector Search\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ayoisio/genai-on-google-cloud/blob/main/chapter-2/colabs/03_embeddings_vector_search.ipynb)\n",
    "\n",
    "**Estimated Time**: 15 minutes\n",
    "\n",
    "**Prerequisites**: Google Cloud project with billing enabled, Vertex AI and BigQuery APIs enabled\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Embeddings transform text into numerical vectors that capture semantic meaning. This notebook demonstrates:\n",
    "\n",
    "1. **Generate embeddings** using Vertex AI text-embedding models\n",
    "2. **Store embeddings** in BigQuery\n",
    "3. **Perform semantic search** using VECTOR_SEARCH\n",
    "4. **Create vector indexes** for efficient retrieval\n",
    "\n",
    "These are the core building blocks for RAG (Retrieval-Augmented Generation) systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install Dependencies\n",
    "!pip install --upgrade google-cloud-aiplatform google-cloud-bigquery -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Authenticate with Google Cloud\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "print(\"‚úì Authentication successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configure Your Project\n",
    "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "DATASET_ID = \"chapter2_demo\"  # @param {type:\"string\"}\n",
    "\n",
    "# Validate project ID\n",
    "if PROJECT_ID == \"your-project-id\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID above\")\n",
    "\n",
    "print(f\"‚úì Project: {PROJECT_ID}\")\n",
    "print(f\"‚úì Location: {LOCATION}\")\n",
    "print(f\"‚úì Dataset: {DATASET_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize Clients\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from google.cloud import bigquery\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "print(f\"‚úì Vertex AI initialized\")\n",
    "print(f\"‚úì BigQuery client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Understanding Embeddings\n\nText embeddings convert words and sentences into dense vectors where semantically similar texts are close together in the vector space.\n\n```mermaid\nflowchart LR\n    A[Text] --> B[Embedding Model]\n    B --> C[Vector]\n    C --> D[Similarity Search]\n    D --> E[Results]\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load the text embedding model\n",
    "# Using text-embedding-005 - the latest model as of December 2025\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\n",
    "\n",
    "print(\"‚úì Loaded text-embedding-005 model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate embeddings for sample texts\n",
    "sample_texts = [\n",
    "    \"How to train a machine learning model\",\n",
    "    \"Best practices for training ML models\",\n",
    "    \"The weather forecast for tomorrow\",\n",
    "    \"Recipe for chocolate chip cookies\",\n",
    "    \"Deep learning neural network architecture\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_model.get_embeddings(sample_texts)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0].values)}\")\n",
    "print(f\"\\nFirst embedding (first 10 values): {embeddings[0].values[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compute similarity between texts\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Get embedding vectors\n",
    "vectors = [np.array(e.values) for e in embeddings]\n",
    "\n",
    "# Compute similarity matrix\n",
    "print(\"Similarity Matrix:\")\n",
    "print(\"(Higher = more similar)\\n\")\n",
    "\n",
    "# Print header\n",
    "print(f\"{'':>40}\", end=\"\")\n",
    "for i in range(len(sample_texts)):\n",
    "    print(f\"  [{i}]\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, text_i in enumerate(sample_texts):\n",
    "    print(f\"[{i}] {text_i[:35]:>35}...\", end=\"\" if len(text_i) > 35 else f\"[{i}] {text_i:>38}\")\n",
    "    for j in range(len(sample_texts)):\n",
    "        sim = cosine_similarity(vectors[i], vectors[j])\n",
    "        print(f\" {sim:.2f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Find most similar text to a query\n",
    "query = \"How do I build an AI model?\"\n",
    "\n",
    "# Get query embedding\n",
    "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "query_vector = np.array(query_embedding)\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = []\n",
    "for i, vec in enumerate(vectors):\n",
    "    sim = cosine_similarity(query_vector, vec)\n",
    "    similarities.append((sample_texts[i], sim))\n",
    "\n",
    "# Sort by similarity\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Most similar texts:\")\n",
    "for text, sim in similarities:\n",
    "    print(f\"  {sim:.4f}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings in BigQuery\n",
    "\n",
    "BigQuery provides native support for embeddings with `ML.GENERATE_EMBEDDING` and `VECTOR_SEARCH` functions. Let's set up a table and generate embeddings at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create dataset if it doesn't exist\n",
    "dataset_ref = bigquery.DatasetReference(PROJECT_ID, DATASET_ID)\n",
    "\n",
    "try:\n",
    "    bq_client.get_dataset(dataset_ref)\n",
    "    print(f\"‚úì Dataset {DATASET_ID} already exists\")\n",
    "except:\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = LOCATION\n",
    "    bq_client.create_dataset(dataset)\n",
    "    print(f\"‚úì Created dataset {DATASET_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create a remote model for embeddings\n",
    "MODEL_NAME = \"text_embedding_model\"\n",
    "\n",
    "create_model_query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{PROJECT_ID}.{DATASET_ID}.{MODEL_NAME}`\n",
    "REMOTE WITH CONNECTION `{PROJECT_ID}.{LOCATION}.default`\n",
    "OPTIONS (\n",
    "    endpoint = 'text-embedding-005'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Note: This requires a BigQuery connection to Vertex AI\n",
    "# If you don't have one, the cell below provides an alternative\n",
    "print(\"Remote Model Creation Query:\")\n",
    "print(create_model_query)\n",
    "print(\"\\n‚ö†Ô∏è Note: This requires a BigQuery-Vertex AI connection.\")\n",
    "print(\"See: https://cloud.google.com/bigquery/docs/create-cloud-resource-connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create sample documents table\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.documents` AS\n",
    "SELECT \n",
    "    'doc_1' as doc_id,\n",
    "    'Machine learning is a subset of artificial intelligence that enables systems to learn from data.' as content\n",
    "UNION ALL SELECT 'doc_2', 'Neural networks are computing systems inspired by biological neural networks.'\n",
    "UNION ALL SELECT 'doc_3', 'Deep learning uses multiple layers of neural networks to analyze data.'\n",
    "UNION ALL SELECT 'doc_4', 'Natural language processing helps computers understand human language.'\n",
    "UNION ALL SELECT 'doc_5', 'Computer vision enables machines to interpret and process visual information.'\n",
    "UNION ALL SELECT 'doc_6', 'Reinforcement learning trains agents through rewards and penalties.'\n",
    "UNION ALL SELECT 'doc_7', 'Transfer learning applies knowledge from one task to another related task.'\n",
    "UNION ALL SELECT 'doc_8', 'Supervised learning uses labeled data to train predictive models.'\n",
    "UNION ALL SELECT 'doc_9', 'Unsupervised learning finds patterns in data without labeled examples.'\n",
    "UNION ALL SELECT 'doc_10', 'Generative AI creates new content like text, images, and code.'\n",
    "\"\"\"\n",
    "\n",
    "bq_client.query(create_table_query).result()\n",
    "print(f\"‚úì Created documents table with sample data\")\n",
    "\n",
    "# Display the data\n",
    "display_query = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.documents`\"\n",
    "df = bq_client.query(display_query).to_dataframe()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate embeddings using Python (alternative to BigQuery ML)\n",
    "# This approach works without a BigQuery-Vertex AI connection\n",
    "\n",
    "# Get the documents\n",
    "documents = df.to_dict('records')\n",
    "\n",
    "# Generate embeddings for each document\n",
    "doc_embeddings = []\n",
    "contents = [doc['content'] for doc in documents]\n",
    "embeddings = embedding_model.get_embeddings(contents)\n",
    "\n",
    "for doc, emb in zip(documents, embeddings):\n",
    "    doc_embeddings.append({\n",
    "        'doc_id': doc['doc_id'],\n",
    "        'content': doc['content'],\n",
    "        'embedding': emb.values\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Generated embeddings for {len(doc_embeddings)} documents\")\n",
    "print(f\"  Embedding dimension: {len(doc_embeddings[0]['embedding'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Store embeddings in BigQuery\n",
    "import json\n",
    "\n",
    "# Create table with ARRAY<FLOAT64> for embeddings\n",
    "create_embeddings_table = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.document_embeddings` (\n",
    "    doc_id STRING,\n",
    "    content STRING,\n",
    "    embedding ARRAY<FLOAT64>\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "bq_client.query(create_embeddings_table).result()\n",
    "print(f\"‚úì Created embeddings table\")\n",
    "\n",
    "# Insert embeddings\n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.document_embeddings\"\n",
    "\n",
    "rows_to_insert = [\n",
    "    {\n",
    "        'doc_id': doc['doc_id'],\n",
    "        'content': doc['content'],\n",
    "        'embedding': doc['embedding']\n",
    "    }\n",
    "    for doc in doc_embeddings\n",
    "]\n",
    "\n",
    "errors = bq_client.insert_rows_json(table_id, rows_to_insert)\n",
    "\n",
    "if errors:\n",
    "    print(f\"‚ùå Errors inserting rows: {errors}\")\n",
    "else:\n",
    "    print(f\"‚úì Inserted {len(rows_to_insert)} rows with embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Verify embeddings in BigQuery\n",
    "verify_query = f\"\"\"\n",
    "SELECT \n",
    "    doc_id,\n",
    "    SUBSTR(content, 1, 50) as content_preview,\n",
    "    ARRAY_LENGTH(embedding) as embedding_dim\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.document_embeddings`\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "result_df = bq_client.query(verify_query).to_dataframe()\n",
    "print(\"Stored embeddings:\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector Search\n",
    "\n",
    "Now let's perform semantic search using the embeddings we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Semantic search function\n",
    "def semantic_search(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform semantic search over the document embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (doc_id, content, similarity) tuples\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_emb = embedding_model.get_embeddings([query])[0].values\n",
    "    query_vector = np.array(query_emb)\n",
    "    \n",
    "    # Calculate similarity with all documents\n",
    "    results = []\n",
    "    for doc in doc_embeddings:\n",
    "        doc_vector = np.array(doc['embedding'])\n",
    "        sim = cosine_similarity(query_vector, doc_vector)\n",
    "        results.append((doc['doc_id'], doc['content'], sim))\n",
    "    \n",
    "    # Sort by similarity and return top_k\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "print(\"‚úì Semantic search function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test semantic search\n",
    "QUERY = \"How do machines learn from data?\"  # @param {type:\"string\"}\n",
    "TOP_K = 5  # @param {type:\"integer\"}\n",
    "\n",
    "results = semantic_search(QUERY, top_k=TOP_K)\n",
    "\n",
    "print(f\"üîç Query: '{QUERY}'\\n\")\n",
    "print(f\"Top {TOP_K} results:\")\n",
    "print(\"-\" * 80)\n",
    "for doc_id, content, sim in results:\n",
    "    print(f\"[{sim:.4f}] {doc_id}: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test with different queries\n",
    "test_queries = [\n",
    "    \"What is deep learning?\",\n",
    "    \"How to understand text with AI?\",\n",
    "    \"Creating new content with AI\",\n",
    "    \"Learning without labels\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = semantic_search(query, top_k=2)\n",
    "    print(f\"\\nüîç '{query}'\")\n",
    "    for doc_id, content, sim in results:\n",
    "        print(f\"   [{sim:.3f}] {content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BigQuery VECTOR_SEARCH (Reference)\n",
    "\n",
    "BigQuery provides native VECTOR_SEARCH for efficient similarity search at scale. Here's the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title BigQuery VECTOR_SEARCH Pattern\n",
    "VECTOR_SEARCH_PATTERN = '''\n",
    "-- Native BigQuery VECTOR_SEARCH pattern\n",
    "-- This requires ML.GENERATE_EMBEDDING with a remote model connection\n",
    "\n",
    "-- Step 1: Create a table with embeddings\n",
    "CREATE OR REPLACE TABLE `{PROJECT}.{DATASET}.embeddings` AS\n",
    "SELECT \n",
    "    doc_id,\n",
    "    content,\n",
    "    ml_generate_embedding_result AS embedding\n",
    "FROM ML.GENERATE_EMBEDDING(\n",
    "    MODEL `{PROJECT}.{DATASET}.embedding_model`,\n",
    "    (SELECT doc_id, content FROM `{PROJECT}.{DATASET}.documents`)\n",
    ");\n",
    "\n",
    "-- Step 2: Create a vector index for efficient search\n",
    "CREATE OR REPLACE VECTOR INDEX my_vector_index\n",
    "ON `{PROJECT}.{DATASET}.embeddings`(embedding)\n",
    "OPTIONS (\n",
    "    index_type = 'IVF',\n",
    "    distance_type = 'COSINE',\n",
    "    ivf_options = '{\"num_lists\": 100}'\n",
    ");\n",
    "\n",
    "-- Step 3: Perform vector search\n",
    "SELECT\n",
    "    base.doc_id,\n",
    "    base.content,\n",
    "    distance\n",
    "FROM VECTOR_SEARCH(\n",
    "    TABLE `{PROJECT}.{DATASET}.embeddings`,\n",
    "    'embedding',\n",
    "    (\n",
    "        SELECT ml_generate_embedding_result AS embedding\n",
    "        FROM ML.GENERATE_EMBEDDING(\n",
    "            MODEL `{PROJECT}.{DATASET}.embedding_model`,\n",
    "            (SELECT 'How do machines learn?' AS content)\n",
    "        )\n",
    "    ),\n",
    "    top_k => 5,\n",
    "    OPTIONS => '{\"fraction_lists_to_search\": 0.1}'\n",
    ")\n",
    "ORDER BY distance;\n",
    "'''\n",
    "\n",
    "print(\"üìã BigQuery VECTOR_SEARCH Pattern:\")\n",
    "print(VECTOR_SEARCH_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title RAG with VECTOR_SEARCH + ML.GENERATE_TEXT Pattern\n",
    "RAG_PATTERN = '''\n",
    "-- Complete RAG pattern: Vector Search + Text Generation\n",
    "-- Combines semantic retrieval with LLM generation\n",
    "\n",
    "SELECT\n",
    "    ml_generate_text_llm_result AS answer\n",
    "FROM ML.GENERATE_TEXT(\n",
    "    MODEL `{PROJECT}.{DATASET}.gemini_model`,\n",
    "    (\n",
    "        SELECT CONCAT(\n",
    "            'Answer the following question using only the context provided.\\n\\n',\n",
    "            'Context:\\n',\n",
    "            STRING_AGG(base.content, '\\n'),\n",
    "            '\\n\\nQuestion: How do machines learn from data?\\n\\nAnswer:'\n",
    "        ) AS prompt\n",
    "        FROM VECTOR_SEARCH(\n",
    "            TABLE `{PROJECT}.{DATASET}.embeddings`,\n",
    "            'embedding',\n",
    "            (SELECT embedding FROM query_embedding),\n",
    "            top_k => 5\n",
    "        )\n",
    "    ),\n",
    "    STRUCT(0.2 AS temperature, 1024 AS max_output_tokens)\n",
    ");\n",
    "'''\n",
    "\n",
    "print(\"üìã RAG Pattern (Vector Search + Generation):\")\n",
    "print(RAG_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try It Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add your own documents and test semantic search\n",
    "\n",
    "# Add new documents\n",
    "new_documents = [\n",
    "    {\"doc_id\": \"custom_1\", \"content\": \"Your custom document content here\"},\n",
    "    {\"doc_id\": \"custom_2\", \"content\": \"Another document to search\"},\n",
    "]\n",
    "\n",
    "# Generate embeddings for new documents\n",
    "new_contents = [doc['content'] for doc in new_documents]\n",
    "new_embeddings = embedding_model.get_embeddings(new_contents)\n",
    "\n",
    "# Add to our document store\n",
    "for doc, emb in zip(new_documents, new_embeddings):\n",
    "    doc_embeddings.append({\n",
    "        'doc_id': doc['doc_id'],\n",
    "        'content': doc['content'],\n",
    "        'embedding': emb.values\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Added {len(new_documents)} new documents\")\n",
    "print(f\"Total documents: {len(doc_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different embedding models\n",
    "# Available models: text-embedding-005, text-multilingual-embedding-002\n",
    "\n",
    "# Try multilingual embeddings\n",
    "try:\n",
    "    multilingual_model = TextEmbeddingModel.from_pretrained(\"text-multilingual-embedding-002\")\n",
    "    \n",
    "    multilingual_texts = [\n",
    "        \"How does machine learning work?\",  # English\n",
    "        \"¬øC√≥mo funciona el aprendizaje autom√°tico?\",  # Spanish\n",
    "        \"Ê©üÊ¢∞Â≠¶Áøí„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å´Ê©üËÉΩ„Åó„Åæ„Åô„ÅãÔºü\",  # Japanese\n",
    "    ]\n",
    "    \n",
    "    ml_embeddings = multilingual_model.get_embeddings(multilingual_texts)\n",
    "    ml_vectors = [np.array(e.values) for e in ml_embeddings]\n",
    "    \n",
    "    print(\"Multilingual similarity:\")\n",
    "    for i, text_i in enumerate(multilingual_texts):\n",
    "        for j, text_j in enumerate(multilingual_texts):\n",
    "            if i < j:\n",
    "                sim = cosine_similarity(ml_vectors[i], ml_vectors[j])\n",
    "                print(f\"  {text_i[:30]}... ‚Üî {text_j[:30]}... = {sim:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load multilingual model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cleanup resources (optional)\n",
    "CLEANUP = False  # @param {type:\"boolean\"}\n",
    "\n",
    "if CLEANUP:\n",
    "    # Delete tables\n",
    "    bq_client.delete_table(f\"{PROJECT_ID}.{DATASET_ID}.documents\", not_found_ok=True)\n",
    "    bq_client.delete_table(f\"{PROJECT_ID}.{DATASET_ID}.document_embeddings\", not_found_ok=True)\n",
    "    print(\"‚úì Deleted tables\")\n",
    "    \n",
    "    # Optionally delete dataset\n",
    "    # bq_client.delete_dataset(f\"{PROJECT_ID}.{DATASET_ID}\", delete_contents=True)\n",
    "    # print(\"‚úì Deleted dataset\")\n",
    "else:\n",
    "    print(\"Skipping cleanup. Set CLEANUP=True to delete resources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. ‚úÖ **Generate embeddings** using Vertex AI text-embedding models\n",
    "2. ‚úÖ **Understand similarity** between texts using cosine similarity\n",
    "3. ‚úÖ **Store embeddings** in BigQuery\n",
    "4. ‚úÖ **Perform semantic search** to find relevant documents\n",
    "5. ‚úÖ **Use BigQuery VECTOR_SEARCH** patterns for scale\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Embeddings** capture semantic meaning in dense vectors\n",
    "- **Cosine similarity** measures how similar texts are\n",
    "- **BigQuery VECTOR_SEARCH** enables efficient search at scale\n",
    "- **Vector indexes** accelerate nearest-neighbor queries\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to the next notebook: **[04_rag_context_assembly.ipynb](04_rag_context_assembly.ipynb)** to learn how to build a complete RAG pipeline with context assembly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}